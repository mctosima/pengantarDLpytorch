{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "vocabulary_size = 20000\n",
    "lr = 0.005\n",
    "batch_size = 128\n",
    "num_epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset\n",
    "[Dataset Link](https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_column</th>\n",
       "      <th>label_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_column  label_column\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...             1\n",
       "1  OK... so... I really like Kris Kristofferson a...             0\n",
       "2  ***SPOILER*** Do not read this, if you think a...             0\n",
       "3  hi for all the people who have seen this wonde...             1\n",
       "4  I recently bought the DVD, forgetting just how...             0\n",
       "5  Leave it to Braik to put on a good show. Final...             1\n",
       "6  Nathan Detroit (Frank Sinatra) is the manager ...             1\n",
       "7  To understand \"Crash Course\" in the right cont...             1\n",
       "8  I've been impressed with Chavez's stance again...             1\n",
       "9  This movie is directed by Renny Harlin the fin...             1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataaset csv disimpan dalam folder `data` dengan lokasi relatif satu level diatas notebook ini\n",
    "dataset_path = '../data/movie_data.csv' \n",
    "df = pd.read_csv(dataset_path)\n",
    "df.columns = ['text_column', 'label_column']\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/movie_data_cleaned.csv', index=False)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mempersiapkan Data\n",
    "\n",
    "**Prasyarat:**\n",
    "- Paket `spacy` harus sudah terinstall di python anda\n",
    "- Anda juga perlu mengunduh vocabulary bahasa inggris dari spacy dengan cara mengetikkan perintah di bawah ini pada terminal anda\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "**Penjelasan:**\n",
    "- Versi `torchtext` yang digunakan dalam tutorial ini adalah `0.6.0`. Jika anda ingin menggunakan `torchtext` versi terbaru, silahkan merujuk pada [standar API baru torchtext](https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb#scrollTo=jXUgsnxw70-M)\n",
    "- Tokenize akan mengubah kalimat pada teks menjadi token. Misalnya : `'Hello world'` menjadi `['Hello', 'world']`\n",
    "- Detail tentang `torchtext.data` dapat dilihat pada [tautan berikut](https://torchtext.readthedocs.io/en/latest/data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_column': ['In', '1974', ',', 'the', 'teenager', 'Martha', 'Moxley', '(', 'Maggie', 'Grace', ')', 'moves', 'to', 'the', 'high', '-', 'class', 'area', 'of', 'Belle', 'Haven', ',', 'Greenwich', ',', 'Connecticut', '.', 'On', 'the', 'Mischief', 'Night', ',', 'eve', 'of', 'Halloween', ',', 'she', 'was', 'murdered', 'in', 'the', 'backyard', 'of', 'her', 'house', 'and', 'her', 'murder', 'remained', 'unsolved', '.', 'Twenty', '-', 'two', 'years', 'later', ',', 'the', 'writer', 'Mark', 'Fuhrman', '(', 'Christopher', 'Meloni', ')', ',', 'who', 'is', 'a', 'former', 'LA', 'detective', 'that', 'has', 'fallen', 'in', 'disgrace', 'for', 'perjury', 'in', 'O.J.', 'Simpson', 'trial', 'and', 'moved', 'to', 'Idaho', ',', 'decides', 'to', 'investigate', 'the', 'case', 'with', 'his', 'partner', 'Stephen', 'Weeks', '(', 'Andrew', 'Mitchell', ')', 'with', 'the', 'purpose', 'of', 'writing', 'a', 'book', '.', 'The', 'locals', 'squirm', 'and', 'do', 'not', 'welcome', 'them', ',', 'but', 'with', 'the', 'support', 'of', 'the', 'retired', 'detective', 'Steve', 'Carroll', '(', 'Robert', 'Forster', ')', 'that', 'was', 'in', 'charge', 'of', 'the', 'investigation', 'in', 'the', '70', \"'s\", ',', 'they', 'discover', 'the', 'criminal', 'and', 'a', 'net', 'of', 'power', 'and', 'money', 'to', 'cover', 'the', 'murder.<br', '/><br', '/>\"Murder', 'in', 'Greenwich', '\"', 'is', 'a', 'good', 'TV', 'movie', ',', 'with', 'the', 'true', 'story', 'of', 'a', 'murder', 'of', 'a', 'fifteen', 'years', 'old', 'girl', 'that', 'was', 'committed', 'by', 'a', 'wealthy', 'teenager', 'whose', 'mother', 'was', 'a', 'Kennedy', '.', 'The', 'powerful', 'and', 'rich', 'family', 'used', 'their', 'influence', 'to', 'cover', 'the', 'murder', 'for', 'more', 'than', 'twenty', 'years', '.', 'However', ',', 'a', 'snoopy', 'detective', 'and', 'convicted', 'perjurer', 'in', 'disgrace', 'was', 'able', 'to', 'disclose', 'how', 'the', 'hideous', 'crime', 'was', 'committed', '.', 'The', 'screenplay', 'shows', 'the', 'investigation', 'of', 'Mark', 'and', 'the', 'last', 'days', 'of', 'Martha', 'in', 'parallel', ',', 'but', 'there', 'is', 'a', 'lack', 'of', 'the', 'emotion', 'in', 'the', 'dramatization', '.', 'My', 'vote', 'is', 'seven.<br', '/><br', '/>Title', '(', 'Brazil', '):', 'Not', 'Available'], 'label_column': '1'}\n"
     ]
    }
   ],
   "source": [
    "text = torchtext.data.Field(\n",
    "    tokenize = 'spacy',\n",
    "    tokenizer_language='en_core_web_sm',\n",
    ")\n",
    "\n",
    "label = torchtext.data.LabelField(dtype=torch.long)\n",
    "\n",
    "fields = [('text_column', text), ('label_column', label)]\n",
    "\n",
    "dataset = torchtext.data.TabularDataset(\n",
    "    path='../data/movie_data_cleaned.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=fields,\n",
    ")\n",
    "\n",
    "print(vars(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 35000\n",
      "Test data size: 10000\n",
      "Validation data size: 5000\n",
      "{'text_column': ['Pros', ':', 'Nothing', '<', 'br', '/><br', '/>Cons', ':', 'Everything', '<', 'br', '/><br', '/>Plot', 'summary', ':', 'A', 'female', 'reporter', 'runs', 'into', 'a', 'hitchhiker', 'that', 'tells', 'her', 'stories', 'about', 'the', 'deaths', 'of', 'people', 'that', 'were', 'killed', 'by', 'zombies.<br', '/><br', '/>Review', ':', 'Never', 'in', 'my', 'life', 'have', 'I', 'come', 'across', 'a', 'movie', 'as', 'bad', 'The', 'Zombie', 'Chronicles', '.', 'Filmed', 'on', 'a', 'budget', 'of', 'what', 'looks', 'to', 'be', 'about', '20', 'bucks', ',', 'TZC', 'is', 'a', 'completely', 'horrible', 'horror', 'movie', 'that', 'relies', 'on', 'lame', ',', 'forgetable', 'actors', 'whom', 'could', \"n't\", 'act', 'to', 'save', 'their', 'lives', 'and', 'gore', 'that', \"'s\", 'more', 'gross', 'than', 'frightening', '.', 'How', 'does', 'a', 'movie', 'like', 'this', 'even', 'get', 'made', '?', 'Simply', 'put', ',', 'avoid', 'TZC', 'like', 'a', 'sexually', '-', 'transmitted', 'disease.<br', '/><br', '/>My', 'last', '2', 'cents', ':', 'Humorously', 'enough', ',', 'this', 'movie', 'was', 'made', 'by', 'a', 'movie', 'company', 'called', 'Brain', 'Damage', 'Films', '.', 'They', \"'re\", 'brains', 'must', 'have', 'really', 'been', 'damaged', 'to', 'come', 'up', 'with', 'a', 'craptacular', 'movie', 'like', 'this.<br', '/><br', '/>My', 'rating', ':', '1', 'out', 'of', '10(If', 'it', 'were', 'up', 'to', 'me', ',', 'this', 'movie', 'would', 'get', 'the', 'rating', 'of', 'negative', 'bajillion', ')'], 'label_column': '0'}\n"
     ]
    }
   ],
   "source": [
    "# train_data, test_data, val_data = random_split(\n",
    "#     dataset,\n",
    "#     [int(len(dataset) * 0.7), int(len(dataset) * 0.2), int(len(dataset) * 0.1)],\n",
    "#     torch.Generator().manual_seed(random_seed),\n",
    "# )\n",
    "\n",
    "train_data, val_data, test_data = dataset.split(\n",
    "    split_ratio=[0.7, 0.2, 0.1],\n",
    "    random_state = random.seed(random_seed),\n",
    ")\n",
    "\n",
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Test data size: {len(test_data)}')\n",
    "print(f'Validation data size: {len(val_data)}')\n",
    "\n",
    "# Mengecek contoh train_data\n",
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membangun Vocabulary / Kamus Kata\n",
    "- Vocabulary dibatasi sebesar 20000 (hanya menampilkan 20000 kata yang paling sering dipakai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Label size: 2\n"
     ]
    }
   ],
   "source": [
    "text.build_vocab(train_data, max_size=vocabulary_size)\n",
    "label.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(text.vocab)}')\n",
    "print(f'Label size: {len(label.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 402761), (',', 381621), ('.', 328787), ('and', 217039), ('a', 216689), ('of', 200379), ('to', 185267), ('is', 150020), ('in', 122410), ('I', 108843), ('it', 106563), ('that', 96538), ('\"', 89116), (\"'s\", 85279), ('this', 84271), ('-', 73508), ('/><br', 70760), ('was', 69368), ('as', 59751), ('movie', 59142)]\n",
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
      "5\n",
      "defaultdict(None, {'1': 0, '0': 1})\n"
     ]
    }
   ],
   "source": [
    "# kata yang paling banyak muncul\n",
    "print(text.vocab.freqs.most_common(20))\n",
    "\n",
    "# 10 entri pertama (integer to string)\n",
    "print(text.vocab.itos[:10])\n",
    "\n",
    "# stoi : string to integer\n",
    "print(text.vocab.stoi['and'])\n",
    "\n",
    "# Label '1' atau positif ada di index 0, sementara label '0' atau negatif ada di indeks 1\n",
    "print(label.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text_column]:[torch.LongTensor of size 946x128]\n",
      "\t[.label_column]:[torch.LongTensor of size 128]\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text_column]:[torch.LongTensor of size 1068x128]\n",
      "\t[.label_column]:[torch.LongTensor of size 128]\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text_column]:[torch.LongTensor of size 60x128]\n",
      "\t[.label_column]:[torch.LongTensor of size 128]\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text_column]:[torch.LongTensor of size 52x128]\n",
      "\t[.label_column]:[torch.LongTensor of size 128]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=False,\n",
    "    sort_key=lambda x: len(x.text_column),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "check_iter = iter(train_loader)\n",
    "print(next(check_iter))\n",
    "print(next(check_iter))\n",
    "\n",
    "check_iter = iter(val_loader)\n",
    "print(next(check_iter))\n",
    "\n",
    "check_iter = iter(test_loader)\n",
    "print(next(check_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        # self.rnn = nn.RNN(embedding_dim, hidden_dim, nonlinearity='relu')\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        output = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(output)\n",
    "        hidden.squeeze_()\n",
    "        final_output = self.fc(hidden)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(\n",
    "    input_size=len(text.vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_size=num_classes\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch: 0/274 | Loss: 0.7115\n",
      "Epoch: 0 | Batch: 10/274 | Loss: 0.6949\n",
      "Epoch: 0 | Batch: 20/274 | Loss: 0.6950\n",
      "Epoch: 0 | Batch: 30/274 | Loss: 0.6930\n",
      "Epoch: 0 | Batch: 40/274 | Loss: 0.6947\n",
      "Epoch: 0 | Batch: 50/274 | Loss: 0.6950\n",
      "Epoch: 0 | Batch: 60/274 | Loss: 0.6917\n",
      "Epoch: 0 | Batch: 70/274 | Loss: 0.6964\n",
      "Epoch: 0 | Batch: 80/274 | Loss: 0.6903\n",
      "Epoch: 0 | Batch: 90/274 | Loss: 0.6921\n",
      "Epoch: 0 | Batch: 100/274 | Loss: 0.6936\n",
      "Epoch: 0 | Batch: 110/274 | Loss: 0.6945\n",
      "Epoch: 0 | Batch: 120/274 | Loss: 0.6910\n",
      "Epoch: 0 | Batch: 130/274 | Loss: 0.6934\n",
      "Epoch: 0 | Batch: 140/274 | Loss: 0.6915\n",
      "Epoch: 0 | Batch: 150/274 | Loss: 0.6932\n",
      "Epoch: 0 | Batch: 160/274 | Loss: 0.6932\n",
      "Epoch: 0 | Batch: 170/274 | Loss: 0.6928\n",
      "Epoch: 0 | Batch: 180/274 | Loss: 0.6946\n",
      "Epoch: 0 | Batch: 190/274 | Loss: 0.6940\n",
      "Epoch: 0 | Batch: 200/274 | Loss: 0.6931\n",
      "Epoch: 0 | Batch: 210/274 | Loss: 0.6901\n",
      "Epoch: 0 | Batch: 220/274 | Loss: 0.6943\n",
      "Epoch: 0 | Batch: 230/274 | Loss: 0.6926\n",
      "Epoch: 0 | Batch: 240/274 | Loss: 0.6934\n",
      "Epoch: 0 | Batch: 250/274 | Loss: 0.6929\n",
      "Epoch: 0 | Batch: 260/274 | Loss: 0.6920\n",
      "Epoch: 0 | Batch: 270/274 | Loss: 0.6931\n",
      "Epoch: 0 | Accuracy: 0.5034\n",
      "Epoch: 1 | Batch: 0/274 | Loss: 0.6912\n",
      "Epoch: 1 | Batch: 10/274 | Loss: 0.6888\n",
      "Epoch: 1 | Batch: 20/274 | Loss: 0.6992\n",
      "Epoch: 1 | Batch: 30/274 | Loss: 0.6951\n",
      "Epoch: 1 | Batch: 40/274 | Loss: 0.6909\n",
      "Epoch: 1 | Batch: 50/274 | Loss: 0.6937\n",
      "Epoch: 1 | Batch: 60/274 | Loss: 0.6893\n",
      "Epoch: 1 | Batch: 70/274 | Loss: 0.6985\n",
      "Epoch: 1 | Batch: 80/274 | Loss: 0.6953\n",
      "Epoch: 1 | Batch: 90/274 | Loss: 0.7007\n",
      "Epoch: 1 | Batch: 100/274 | Loss: 0.6927\n",
      "Epoch: 1 | Batch: 110/274 | Loss: 0.6902\n",
      "Epoch: 1 | Batch: 120/274 | Loss: 0.6935\n",
      "Epoch: 1 | Batch: 130/274 | Loss: 0.6954\n",
      "Epoch: 1 | Batch: 140/274 | Loss: 0.6966\n",
      "Epoch: 1 | Batch: 150/274 | Loss: 0.6912\n",
      "Epoch: 1 | Batch: 160/274 | Loss: 0.6937\n",
      "Epoch: 1 | Batch: 170/274 | Loss: 0.6970\n",
      "Epoch: 1 | Batch: 180/274 | Loss: 0.6925\n",
      "Epoch: 1 | Batch: 190/274 | Loss: 0.6938\n",
      "Epoch: 1 | Batch: 200/274 | Loss: 0.6967\n",
      "Epoch: 1 | Batch: 210/274 | Loss: 0.7007\n",
      "Epoch: 1 | Batch: 220/274 | Loss: 0.6961\n",
      "Epoch: 1 | Batch: 230/274 | Loss: 0.6998\n",
      "Epoch: 1 | Batch: 240/274 | Loss: 0.6926\n",
      "Epoch: 1 | Batch: 250/274 | Loss: 0.6917\n",
      "Epoch: 1 | Batch: 260/274 | Loss: 0.6939\n",
      "Epoch: 1 | Batch: 270/274 | Loss: 0.6923\n",
      "Epoch: 1 | Accuracy: 0.5056\n",
      "Epoch: 2 | Batch: 0/274 | Loss: 0.6913\n",
      "Epoch: 2 | Batch: 10/274 | Loss: 0.6945\n",
      "Epoch: 2 | Batch: 20/274 | Loss: 0.6869\n",
      "Epoch: 2 | Batch: 30/274 | Loss: 0.7021\n",
      "Epoch: 2 | Batch: 40/274 | Loss: 0.6917\n",
      "Epoch: 2 | Batch: 50/274 | Loss: 0.6880\n",
      "Epoch: 2 | Batch: 60/274 | Loss: 0.6888\n",
      "Epoch: 2 | Batch: 70/274 | Loss: 0.6907\n",
      "Epoch: 2 | Batch: 80/274 | Loss: 0.7003\n",
      "Epoch: 2 | Batch: 90/274 | Loss: 0.7069\n",
      "Epoch: 2 | Batch: 100/274 | Loss: 0.6908\n",
      "Epoch: 2 | Batch: 110/274 | Loss: 0.6935\n",
      "Epoch: 2 | Batch: 120/274 | Loss: 0.6910\n",
      "Epoch: 2 | Batch: 130/274 | Loss: 0.6897\n",
      "Epoch: 2 | Batch: 140/274 | Loss: 0.6920\n",
      "Epoch: 2 | Batch: 150/274 | Loss: 0.6924\n",
      "Epoch: 2 | Batch: 160/274 | Loss: 0.6901\n",
      "Epoch: 2 | Batch: 170/274 | Loss: 0.6962\n",
      "Epoch: 2 | Batch: 180/274 | Loss: 0.6910\n",
      "Epoch: 2 | Batch: 190/274 | Loss: 0.6878\n",
      "Epoch: 2 | Batch: 200/274 | Loss: 0.6956\n",
      "Epoch: 2 | Batch: 210/274 | Loss: 0.6894\n",
      "Epoch: 2 | Batch: 220/274 | Loss: 0.6926\n",
      "Epoch: 2 | Batch: 230/274 | Loss: 0.6882\n",
      "Epoch: 2 | Batch: 240/274 | Loss: 0.6898\n",
      "Epoch: 2 | Batch: 250/274 | Loss: 0.6993\n",
      "Epoch: 2 | Batch: 260/274 | Loss: 0.6882\n",
      "Epoch: 2 | Batch: 270/274 | Loss: 0.6918\n",
      "Epoch: 2 | Accuracy: 0.5018\n",
      "Epoch: 3 | Batch: 0/274 | Loss: 0.6918\n",
      "Epoch: 3 | Batch: 10/274 | Loss: 0.6897\n",
      "Epoch: 3 | Batch: 20/274 | Loss: 0.6913\n",
      "Epoch: 3 | Batch: 30/274 | Loss: 0.6928\n",
      "Epoch: 3 | Batch: 40/274 | Loss: 0.6876\n",
      "Epoch: 3 | Batch: 50/274 | Loss: 0.6898\n",
      "Epoch: 3 | Batch: 60/274 | Loss: 0.7003\n",
      "Epoch: 3 | Batch: 70/274 | Loss: 0.6893\n",
      "Epoch: 3 | Batch: 80/274 | Loss: 0.6968\n",
      "Epoch: 3 | Batch: 90/274 | Loss: 0.6910\n",
      "Epoch: 3 | Batch: 100/274 | Loss: 0.6886\n",
      "Epoch: 3 | Batch: 110/274 | Loss: 0.6885\n",
      "Epoch: 3 | Batch: 120/274 | Loss: 0.6903\n",
      "Epoch: 3 | Batch: 130/274 | Loss: 0.6889\n",
      "Epoch: 3 | Batch: 140/274 | Loss: 0.6928\n",
      "Epoch: 3 | Batch: 150/274 | Loss: 0.6929\n",
      "Epoch: 3 | Batch: 160/274 | Loss: 0.7057\n",
      "Epoch: 3 | Batch: 170/274 | Loss: 0.6893\n",
      "Epoch: 3 | Batch: 180/274 | Loss: 0.6913\n",
      "Epoch: 3 | Batch: 190/274 | Loss: 0.6890\n",
      "Epoch: 3 | Batch: 200/274 | Loss: 0.6905\n",
      "Epoch: 3 | Batch: 210/274 | Loss: 0.7066\n",
      "Epoch: 3 | Batch: 220/274 | Loss: 0.6899\n",
      "Epoch: 3 | Batch: 230/274 | Loss: 0.6934\n",
      "Epoch: 3 | Batch: 240/274 | Loss: 0.6881\n",
      "Epoch: 3 | Batch: 250/274 | Loss: 0.6830\n",
      "Epoch: 3 | Batch: 260/274 | Loss: 0.6960\n",
      "Epoch: 3 | Batch: 270/274 | Loss: 0.6957\n",
      "Epoch: 3 | Accuracy: 0.5038\n",
      "Epoch: 4 | Batch: 0/274 | Loss: 0.6902\n",
      "Epoch: 4 | Batch: 10/274 | Loss: 0.6905\n",
      "Epoch: 4 | Batch: 20/274 | Loss: 0.6990\n",
      "Epoch: 4 | Batch: 30/274 | Loss: 0.6969\n",
      "Epoch: 4 | Batch: 40/274 | Loss: 0.6896\n",
      "Epoch: 4 | Batch: 50/274 | Loss: 0.6894\n",
      "Epoch: 4 | Batch: 60/274 | Loss: 0.6885\n",
      "Epoch: 4 | Batch: 70/274 | Loss: 0.6914\n",
      "Epoch: 4 | Batch: 80/274 | Loss: 0.6940\n",
      "Epoch: 4 | Batch: 90/274 | Loss: 0.6931\n",
      "Epoch: 4 | Batch: 100/274 | Loss: 0.6894\n",
      "Epoch: 4 | Batch: 110/274 | Loss: 0.6883\n",
      "Epoch: 4 | Batch: 120/274 | Loss: 0.6949\n",
      "Epoch: 4 | Batch: 130/274 | Loss: 0.6936\n",
      "Epoch: 4 | Batch: 140/274 | Loss: 0.6968\n",
      "Epoch: 4 | Batch: 150/274 | Loss: 0.6986\n",
      "Epoch: 4 | Batch: 160/274 | Loss: 0.6907\n",
      "Epoch: 4 | Batch: 170/274 | Loss: 0.6978\n",
      "Epoch: 4 | Batch: 180/274 | Loss: 0.6897\n",
      "Epoch: 4 | Batch: 190/274 | Loss: 0.6940\n",
      "Epoch: 4 | Batch: 200/274 | Loss: 0.6899\n",
      "Epoch: 4 | Batch: 210/274 | Loss: 0.6931\n",
      "Epoch: 4 | Batch: 220/274 | Loss: 0.6917\n",
      "Epoch: 4 | Batch: 230/274 | Loss: 0.6910\n",
      "Epoch: 4 | Batch: 240/274 | Loss: 0.6903\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB Code/20_rnn_imdb.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000016?line=7'>8</a>\u001b[0m text \u001b[39m=\u001b[39m batch_data\u001b[39m.\u001b[39mtext_column\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000016?line=8'>9</a>\u001b[0m labels \u001b[39m=\u001b[39m batch_data\u001b[39m.\u001b[39mlabel_column\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000016?line=10'>11</a>\u001b[0m logits \u001b[39m=\u001b[39m model(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000016?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000016?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB Code/20_rnn_imdb.ipynb Cell 15'\u001b[0m in \u001b[0;36mRNN.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000014?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000014?line=11'>12</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(text)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000014?line=12'>13</a>\u001b[0m     output, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000014?line=13'>14</a>\u001b[0m     hidden\u001b[39m.\u001b[39msqueeze_()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000014?line=14'>15</a>\u001b[0m     final_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(hidden)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=688'>689</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=689'>690</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=690'>691</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=691'>692</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=692'>693</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=693'>694</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=694'>695</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text = batch_data.text_column.to(device)\n",
    "        labels = batch_data.label_column.to(device)\n",
    "        \n",
    "        logits = model(text)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch} | Batch: {batch_idx}/{len(train_loader)} | Loss: {loss:.4f}')\n",
    "            \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        prediksi_benar = 0\n",
    "        jumlah_example = 0\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(val_loader):\n",
    "            \n",
    "            text = batch_data.text_column.to(device)\n",
    "            labels = batch_data.label_column.to(device)\n",
    "            \n",
    "            logits = model(text)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            \n",
    "            jumlah_example += len(preds)\n",
    "            prediksi_benar += (preds == labels).sum().item()\n",
    "            \n",
    "        print(f'Epoch: {epoch} | Accuracy: {prediksi_benar / jumlah_example}')\n",
    "\n",
    "print(f'Train time: {time.time() - train_start_time}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81b9ccaf5ef21c8a6faa6d42f6e42fcf9eafd7625a2befdd601079168fccee32"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
