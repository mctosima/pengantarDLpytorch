{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "vocabulary_size = 20000\n",
    "lr = 0.005\n",
    "batch_size = 128\n",
    "num_epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset\n",
    "[Dataset Link](https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_column</th>\n",
       "      <th>label_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I've been impressed with Chavez's stance again...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_column  label_column\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...             1\n",
       "1  OK... so... I really like Kris Kristofferson a...             0\n",
       "2  ***SPOILER*** Do not read this, if you think a...             0\n",
       "3  hi for all the people who have seen this wonde...             1\n",
       "4  I recently bought the DVD, forgetting just how...             0\n",
       "5  Leave it to Braik to put on a good show. Final...             1\n",
       "6  Nathan Detroit (Frank Sinatra) is the manager ...             1\n",
       "7  To understand \"Crash Course\" in the right cont...             1\n",
       "8  I've been impressed with Chavez's stance again...             1\n",
       "9  This movie is directed by Renny Harlin the fin...             1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '../data/movie_data.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.columns = ['text_column', 'label_column']\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/movie_data_cleaned.csv', index=False)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mempersiapkan Data\n",
    "\n",
    "**Prasyarat:**\n",
    "- Paket `spacy` harus sudah terinstall di python anda\n",
    "- Anda juga perlu mengunduh vocabulary bahasa inggris dari spacy dengan cara mengetikkan perintah di bawah ini pada terminal anda\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "**Penjelasan:**\n",
    "- Jika menggunakan `torchtext` > 0.9 maka perlu menambahkan `legacy` pada setiap modul `torchtext.data.`.  Misalnya : `torchtext.legacy.data.Field` atau `torchtext.legacy.data.LabelField()`\n",
    "- Tokenize akan mengubah kalimat pada teks menjadi token. Misalnya : `'Hello world'` menjadi `['Hello', 'world']`\n",
    "- Detail tentang `torchtext.data` dapat dilihat pada [tautan berikut](https://torchtext.readthedocs.io/en/latest/data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_column': ['In', '1974', ',', 'the', 'teenager', 'Martha', 'Moxley', '(', 'Maggie', 'Grace', ')', 'moves', 'to', 'the', 'high', '-', 'class', 'area', 'of', 'Belle', 'Haven', ',', 'Greenwich', ',', 'Connecticut', '.', 'On', 'the', 'Mischief', 'Night', ',', 'eve', 'of', 'Halloween', ',', 'she', 'was', 'murdered', 'in', 'the', 'backyard', 'of', 'her', 'house', 'and', 'her', 'murder', 'remained', 'unsolved', '.', 'Twenty', '-', 'two', 'years', 'later', ',', 'the', 'writer', 'Mark', 'Fuhrman', '(', 'Christopher', 'Meloni', ')', ',', 'who', 'is', 'a', 'former', 'LA', 'detective', 'that', 'has', 'fallen', 'in', 'disgrace', 'for', 'perjury', 'in', 'O.J.', 'Simpson', 'trial', 'and', 'moved', 'to', 'Idaho', ',', 'decides', 'to', 'investigate', 'the', 'case', 'with', 'his', 'partner', 'Stephen', 'Weeks', '(', 'Andrew', 'Mitchell', ')', 'with', 'the', 'purpose', 'of', 'writing', 'a', 'book', '.', 'The', 'locals', 'squirm', 'and', 'do', 'not', 'welcome', 'them', ',', 'but', 'with', 'the', 'support', 'of', 'the', 'retired', 'detective', 'Steve', 'Carroll', '(', 'Robert', 'Forster', ')', 'that', 'was', 'in', 'charge', 'of', 'the', 'investigation', 'in', 'the', '70', \"'s\", ',', 'they', 'discover', 'the', 'criminal', 'and', 'a', 'net', 'of', 'power', 'and', 'money', 'to', 'cover', 'the', 'murder.<br', '/><br', '/>\"Murder', 'in', 'Greenwich', '\"', 'is', 'a', 'good', 'TV', 'movie', ',', 'with', 'the', 'true', 'story', 'of', 'a', 'murder', 'of', 'a', 'fifteen', 'years', 'old', 'girl', 'that', 'was', 'committed', 'by', 'a', 'wealthy', 'teenager', 'whose', 'mother', 'was', 'a', 'Kennedy', '.', 'The', 'powerful', 'and', 'rich', 'family', 'used', 'their', 'influence', 'to', 'cover', 'the', 'murder', 'for', 'more', 'than', 'twenty', 'years', '.', 'However', ',', 'a', 'snoopy', 'detective', 'and', 'convicted', 'perjurer', 'in', 'disgrace', 'was', 'able', 'to', 'disclose', 'how', 'the', 'hideous', 'crime', 'was', 'committed', '.', 'The', 'screenplay', 'shows', 'the', 'investigation', 'of', 'Mark', 'and', 'the', 'last', 'days', 'of', 'Martha', 'in', 'parallel', ',', 'but', 'there', 'is', 'a', 'lack', 'of', 'the', 'emotion', 'in', 'the', 'dramatization', '.', 'My', 'vote', 'is', 'seven.<br', '/><br', '/>Title', '(', 'Brazil', '):', 'Not', 'Available'], 'label_column': '1'}\n"
     ]
    }
   ],
   "source": [
    "text = torchtext.data.Field(\n",
    "    tokenize = 'spacy',\n",
    "    tokenizer_language='en_core_web_sm',\n",
    ")\n",
    "\n",
    "label = torchtext.data.LabelField(dtype=torch.long)\n",
    "\n",
    "fields = [('text_column', text), ('label_column', label)]\n",
    "\n",
    "dataset = torchtext.data.TabularDataset(\n",
    "    path='../data/movie_data_cleaned.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=fields,\n",
    ")\n",
    "\n",
    "print(vars(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 35000\n",
      "Test data size: 10000\n",
      "Validation data size: 5000\n",
      "{'text_column': ['Pros', ':', 'Nothing', '<', 'br', '/><br', '/>Cons', ':', 'Everything', '<', 'br', '/><br', '/>Plot', 'summary', ':', 'A', 'female', 'reporter', 'runs', 'into', 'a', 'hitchhiker', 'that', 'tells', 'her', 'stories', 'about', 'the', 'deaths', 'of', 'people', 'that', 'were', 'killed', 'by', 'zombies.<br', '/><br', '/>Review', ':', 'Never', 'in', 'my', 'life', 'have', 'I', 'come', 'across', 'a', 'movie', 'as', 'bad', 'The', 'Zombie', 'Chronicles', '.', 'Filmed', 'on', 'a', 'budget', 'of', 'what', 'looks', 'to', 'be', 'about', '20', 'bucks', ',', 'TZC', 'is', 'a', 'completely', 'horrible', 'horror', 'movie', 'that', 'relies', 'on', 'lame', ',', 'forgetable', 'actors', 'whom', 'could', \"n't\", 'act', 'to', 'save', 'their', 'lives', 'and', 'gore', 'that', \"'s\", 'more', 'gross', 'than', 'frightening', '.', 'How', 'does', 'a', 'movie', 'like', 'this', 'even', 'get', 'made', '?', 'Simply', 'put', ',', 'avoid', 'TZC', 'like', 'a', 'sexually', '-', 'transmitted', 'disease.<br', '/><br', '/>My', 'last', '2', 'cents', ':', 'Humorously', 'enough', ',', 'this', 'movie', 'was', 'made', 'by', 'a', 'movie', 'company', 'called', 'Brain', 'Damage', 'Films', '.', 'They', \"'re\", 'brains', 'must', 'have', 'really', 'been', 'damaged', 'to', 'come', 'up', 'with', 'a', 'craptacular', 'movie', 'like', 'this.<br', '/><br', '/>My', 'rating', ':', '1', 'out', 'of', '10(If', 'it', 'were', 'up', 'to', 'me', ',', 'this', 'movie', 'would', 'get', 'the', 'rating', 'of', 'negative', 'bajillion', ')'], 'label_column': '0'}\n"
     ]
    }
   ],
   "source": [
    "# train_data, test_data, val_data = random_split(\n",
    "#     dataset,\n",
    "#     [int(len(dataset) * 0.7), int(len(dataset) * 0.2), int(len(dataset) * 0.1)],\n",
    "#     torch.Generator().manual_seed(random_seed),\n",
    "# )\n",
    "\n",
    "train_data, val_data, test_data = dataset.split(\n",
    "    split_ratio=[0.7, 0.2, 0.1],\n",
    "    random_state = random.seed(random_seed),\n",
    ")\n",
    "\n",
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Test data size: {len(test_data)}')\n",
    "print(f'Validation data size: {len(val_data)}')\n",
    "\n",
    "# Mengecek contoh train_data\n",
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membangun Vocabulary / Kamus Kata\n",
    "- Vocabulary dibatasi sebesar 20000 (hanya menampilkan 20000 kata yang paling sering dipakai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Label size: 2\n"
     ]
    }
   ],
   "source": [
    "text.build_vocab(train_data, max_size=vocabulary_size)\n",
    "label.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(text.vocab)}')\n",
    "print(f'Label size: {len(label.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 402761), (',', 381621), ('.', 328787), ('and', 217039), ('a', 216689), ('of', 200379), ('to', 185267), ('is', 150020), ('in', 122410), ('I', 108843), ('it', 106563), ('that', 96538), ('\"', 89116), (\"'s\", 85279), ('this', 84271), ('-', 73508), ('/><br', 70760), ('was', 69368), ('as', 59751), ('movie', 59142)]\n",
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
      "5\n",
      "defaultdict(None, {'1': 0, '0': 1})\n"
     ]
    }
   ],
   "source": [
    "# kata yang paling banyak muncul\n",
    "print(text.vocab.freqs.most_common(20))\n",
    "\n",
    "# 10 entri pertama (integer to string)\n",
    "print(text.vocab.itos[:10])\n",
    "\n",
    "# stoi : string to integer\n",
    "print(text.vocab.stoi['and'])\n",
    "\n",
    "# Label '1' atau positif ada di index 0, sementara label '0' atau negatif ada di indeks 1\n",
    "print(label.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text_column]:[torch.LongTensor of size 946x128]\n",
      "\t[.label_column]:[torch.LongTensor of size 128]\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text_column]:[torch.LongTensor of size 1068x128]\n",
      "\t[.label_column]:[torch.LongTensor of size 128]\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text_column]:[torch.LongTensor of size 60x128]\n",
      "\t[.label_column]:[torch.LongTensor of size 128]\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text_column]:[torch.LongTensor of size 52x128]\n",
      "\t[.label_column]:[torch.LongTensor of size 128]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=False,\n",
    "    sort_key=lambda x: len(x.text_column),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "check_iter = iter(train_loader)\n",
    "print(next(check_iter))\n",
    "print(next(check_iter))\n",
    "\n",
    "check_iter = iter(val_loader)\n",
    "print(next(check_iter))\n",
    "\n",
    "check_iter = iter(test_loader)\n",
    "print(next(check_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        output = self.embedding(text)\n",
    "        output, hidden = self.rnn(output)\n",
    "        hidden.squeeze_()\n",
    "        final_output = self.fc(hidden)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(\n",
    "    input_size=len(text.vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_size=num_classes\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch: 0/274 | Loss: 0.6975\n",
      "Epoch: 0 | Batch: 10/274 | Loss: 0.6872\n",
      "Epoch: 0 | Batch: 20/274 | Loss: 0.6981\n",
      "Epoch: 0 | Batch: 30/274 | Loss: 0.6916\n",
      "Epoch: 0 | Batch: 40/274 | Loss: 0.6955\n",
      "Epoch: 0 | Batch: 50/274 | Loss: 0.6895\n",
      "Epoch: 0 | Batch: 60/274 | Loss: 0.6899\n",
      "Epoch: 0 | Batch: 70/274 | Loss: 0.7072\n",
      "Epoch: 0 | Batch: 80/274 | Loss: 0.6895\n",
      "Epoch: 0 | Batch: 90/274 | Loss: 0.6929\n",
      "Epoch: 0 | Batch: 100/274 | Loss: 0.6988\n",
      "Epoch: 0 | Batch: 110/274 | Loss: 0.6967\n",
      "Epoch: 0 | Batch: 120/274 | Loss: 0.6879\n",
      "Epoch: 0 | Batch: 130/274 | Loss: 0.6968\n",
      "Epoch: 0 | Batch: 140/274 | Loss: 0.6912\n",
      "Epoch: 0 | Batch: 150/274 | Loss: 0.6904\n",
      "Epoch: 0 | Batch: 160/274 | Loss: 0.6946\n",
      "Epoch: 0 | Batch: 170/274 | Loss: 0.6952\n",
      "Epoch: 0 | Batch: 180/274 | Loss: 0.7038\n",
      "Epoch: 0 | Batch: 190/274 | Loss: 0.6898\n",
      "Epoch: 0 | Batch: 200/274 | Loss: 0.6884\n",
      "Epoch: 0 | Batch: 210/274 | Loss: 0.6922\n",
      "Epoch: 0 | Batch: 220/274 | Loss: 0.6950\n",
      "Epoch: 0 | Batch: 230/274 | Loss: 0.6914\n",
      "Epoch: 0 | Batch: 240/274 | Loss: 0.7000\n",
      "Epoch: 0 | Batch: 250/274 | Loss: 0.6924\n",
      "Epoch: 0 | Batch: 260/274 | Loss: 108382996254853079880433664.0000\n",
      "Epoch: 0 | Batch: 270/274 | Loss: nan\n",
      "Epoch: 0 | Accuracy: 0.4992\n",
      "Epoch: 1 | Batch: 0/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 10/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 20/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 30/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 40/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 50/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 60/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 70/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 80/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 90/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 100/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 110/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 120/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 130/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 140/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 150/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 160/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 170/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 180/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 190/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 200/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 210/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 220/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 230/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 240/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 250/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 260/274 | Loss: nan\n",
      "Epoch: 1 | Batch: 270/274 | Loss: nan\n",
      "Epoch: 1 | Accuracy: 0.4992\n",
      "Epoch: 2 | Batch: 0/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 10/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 20/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 30/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 40/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 50/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 60/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 70/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 80/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 90/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 100/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 110/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 120/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 130/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 140/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 150/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 160/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 170/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 180/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 190/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 200/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 210/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 220/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 230/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 240/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 250/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 260/274 | Loss: nan\n",
      "Epoch: 2 | Batch: 270/274 | Loss: nan\n",
      "Epoch: 2 | Accuracy: 0.4992\n",
      "Epoch: 3 | Batch: 0/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 10/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 20/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 30/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 40/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 50/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 60/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 70/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 80/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 90/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 100/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 110/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 120/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 130/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 140/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 150/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 160/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 170/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 180/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 190/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 200/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 210/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 220/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 230/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 240/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 250/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 260/274 | Loss: nan\n",
      "Epoch: 3 | Batch: 270/274 | Loss: nan\n",
      "Epoch: 3 | Accuracy: 0.4992\n",
      "Epoch: 4 | Batch: 0/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 10/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 20/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 30/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 40/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 50/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 60/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 70/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 80/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 90/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 100/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 110/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 120/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 130/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 140/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 150/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 160/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 170/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 180/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 190/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 200/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 210/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 220/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 230/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 240/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 250/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 260/274 | Loss: nan\n",
      "Epoch: 4 | Batch: 270/274 | Loss: nan\n",
      "Epoch: 4 | Accuracy: 0.4992\n",
      "Epoch: 5 | Batch: 0/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 10/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 20/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 30/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 40/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 50/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 60/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 70/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 80/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 90/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 100/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 110/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 120/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 130/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 140/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 150/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 160/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 170/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 180/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 190/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 200/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 210/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 220/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 230/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 240/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 250/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 260/274 | Loss: nan\n",
      "Epoch: 5 | Batch: 270/274 | Loss: nan\n",
      "Epoch: 5 | Accuracy: 0.4992\n",
      "Epoch: 6 | Batch: 0/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 10/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 20/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 30/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 40/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 50/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 60/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 70/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 80/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 90/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 100/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 110/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 120/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 130/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 140/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 150/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 160/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 170/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 180/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 190/274 | Loss: nan\n",
      "Epoch: 6 | Batch: 200/274 | Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB Code/20_rnn_imdb.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000026?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000026?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000026?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000026?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martinmanullang/Developer/pengantarDLpytorch/IPYNB%20Code/20_rnn_imdb.ipynb#ch0000026?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/martinmanullang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text = batch_data.text_column.to(device)\n",
    "        labels = batch_data.label_column.to(device)\n",
    "        \n",
    "        logits = model(text)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch} | Batch: {batch_idx}/{len(train_loader)} | Loss: {loss:.4f}')\n",
    "            \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        prediksi_benar = 0\n",
    "        jumlah_example = 0\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(val_loader):\n",
    "            \n",
    "            text = batch_data.text_column.to(device)\n",
    "            labels = batch_data.label_column.to(device)\n",
    "            \n",
    "            logits = model(text)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            \n",
    "            jumlah_example += len(preds)\n",
    "            prediksi_benar += (preds == labels).sum().item()\n",
    "            \n",
    "        print(f'Epoch: {epoch} | Accuracy: {prediksi_benar / jumlah_example}')\n",
    "\n",
    "print(f'Train time: {time.time() - train_start_time}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81b9ccaf5ef21c8a6faa6d42f6e42fcf9eafd7625a2befdd601079168fccee32"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
