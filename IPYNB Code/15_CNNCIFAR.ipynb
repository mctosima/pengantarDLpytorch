{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Neural Network\n",
    "**Sekilas tentang CNN**\n",
    "Convolutional Neural Network (CNN) adalah salah satu jenis neural network yang biasa digunakan pada data image. CNN bisa digunakan untuk mendeteksi dan mengenali object pada sebuah image. CNN adalah sebuah teknik yang terinspirasi dari cara mamalia â€” manusia, menghasilkan persepsi visual seperti contoh diatas. ([Sumber](https://medium.com/@16611110/apa-itu-convolutional-neural-network-836f70b193a4))\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"../assets/typicalcnn.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "[Sumber Gambar](https://www.quora.com/What-is-a-typical-convolutional-neural-network-CNN-architecture)\n",
    "<br>\n",
    "Pada umumnya, layer pada CNN akan tampak seperti gambar di atas:\n",
    "- Setelah input, akan dilajutkan dengan layer konvolusi yang berisi filter\n",
    "- Setelah layer konvolusi, umumnya akan dilanjutkan dengan pooling layer\n",
    "- Fully connected layer akan digunakan untuk menghasilkan output dan diletakkan pada bagian akhir\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"../assets/convfilter.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "[Sumber Gambar](https://www.researchgate.net/publication/329241581_Using_deep_learning_to_predict_soil_properties_from_regional_spectral_data/figures?lo=1)\n",
    "\n",
    "Filter akan dikonvolusikan terhadap input sehingga menghasilkan sebuah hasil yang ukurannya sama dengan besarnya filter yang digunakan\n",
    "\n",
    "**Max Pooling**\n",
    "Pooling merupakan pengurangan ukuran matriks dengan menggunakan operasi pooling. Pooling layer terdiri dari filter dengan ukuran dan stride tertentu yang berkonvolusi pada seluruh area feature map. Terdapa average pooling dan max pooling. Pada average pooling, nilai yang dihasilkan berdasarkan rata-rata, sementara pada max pooling, nilai yang dihasilkan adalah nilai maksimal. Lapisan Pooling yang dimasukkan diantara lapisan konvolusi secara berturut-turut dalam arsitektur model CNN dapat secara progresif mengurangi ukuran volume output pada Feature Map, sehingga mengurangi jumlah parameter dan perhitungan di jaringan, untuk mengendalikan Overfitting.([Sumber](https://medium.com/@mukhlishatunnada02/kegunaan-layar-pooling-pada-penerapan-deep-learning-menggunakan-convolutional-neural-network-140146078f28))\n",
    "\n",
    "Berikut ini adalah contoh gambar operasi max-pooling :\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"../assets/pool_layer.png\" width=\"400\"/>\n",
    "</div>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tahap Persiapan\n",
    "**Mengimpor Library**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Konfigurasi Device dan Hyperparameter**\n",
    "- Apabila GPU tersedia, gunakan GPU\n",
    "- Jika GPU tidak tersedia, gunakan CPU\n",
    "- Jumlah epoch, batch, dan learning rate di definisikan pada bagian ini"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Konfigurasi Transform**\n",
    "- Pada percobaan ini, dataset CIFAR merupakan PILImage dengan rentang nilai pixel 0-1.\n",
    "- Transformasi dilakukan untuk tipe data menjadi tensor\n",
    "- Normalisasi dilakukan untuk mengubah nilai pixel menjadi 0-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Mengimpor Dataset**\n",
    "- Dataset tersedia melalui fungsi ```torchvision.datasets.CIFAR10``` dengan beberapa penjelasan argumen sebagai berikut\n",
    "    - ```root```: direktori dataset\n",
    "    - ```train```: boolean, jika True, maka dataset akan dijadikan training set\n",
    "    - ```transform```: transformasi yang dilakukan pada dataset\n",
    "    - ```download```: boolean, jika True, maka dataset akan diunduh dari internet\n",
    "- Dataset CIFAR10 ini merupakan dataset yang berisi 10 kategori dengan data training dan testing\n",
    "- ```trainloader``` digunakan untuk memuat data training pada variabel ```trainset```\n",
    "- ```testloader``` digunakan untuk memuat data testing pada variabel ```testset```\n",
    "- argumen ```batch_size``` digunakan untuk mengatur jumlah data yang dibaca per batch\n",
    "- argumen ```shuffle``` digunakan untuk mengatur apakah data akan diacak atau tidak\n",
    "- argumen ```num_workers``` digunakan untuk mengatur jumlah thread yang digunakan untuk membaca data\n",
    "    - Perlu diperhatikan bahwa jupyter notebook tidak mendukung multi-threading, jadi jika menggunakan multi-threading, maka harus menggunakan ```torch.multiprocessing.Pool```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Menginisiasi Kelas**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tahap Mendesain Model\n",
    "Model yang akan digunakan adalah CNN dengan struktur seperti gambar berikut:\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"../assets/cnnmodels.jpeg\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "([Sumber Gambar: mathworks.com](https://www.mathworks.com/discovery/convolutional-neural-network-matlab.html))\n",
    "\n",
    "- Arsitektur ini memiliki dua komponen utama yaitu feature learning dan classifier\n",
    "- Feature learning akan mempelajari fitur gambar dan menghasilkan output yang dapat digunakan untuk classifier\n",
    "- Classifier yang nantinya akan menentukan gambar input masuk ke kelas yang mana\n",
    "\n",
    "**Komponen Layer**\n",
    "- Layer 1: Convolutional Layer 2 Dimensi dengan ukuran input 3 (RGB channel), ukuran output adalah 6, dan ukuran kernel adalah 5\n",
    "- Layer 2: Max Pooling Layer 2 Dimensi dengan ukuran kernel 2 dan stride 2\n",
    "- Layer 3: Convolutional Layer 2 Dimensi dengan ukuran channel 6, ukuran output adalah 16, dan ukuran kernel adalah 5. Ukuran input adalah output dari layer 1\n",
    "- Layer 4: Max Pooling Layer 2 Dimensi dengan ukuran kernel 2 dan stride 2 (sama seperti layer 2)\n",
    "- Layer 5: Image diflatten dan dimasukkan ke Fully Connected Layer dengan ukuran input 16 * 5 * 5, ukuran output adalah 120\n",
    "- Layer 6: Fully connected layer dengan ukuran input 120, ukuran output adalah 84\n",
    "- Layer 7: Fully connected layer dengan ukuran input 84, ukuran output adalah 10 (jumlah kelas)\n",
    "\n",
    "**Perubahan shape image**\n",
    "- Pada awalnya, gambar input adalah 3 dimensi (RGB) dengan ukuran 32x32 piksel\n",
    "- Setelah melewati layer 1, gambar akan berubah menjadi 6 dimensi (RGB) dengan ukuran 28x28 piksel. Mengapa menjadi 28 piksel?\n",
    "- Formula Conv Layer: $$\\frac{(W - F + 2*P)}{S} + 1$$\n",
    "- Width (W) awal adalah 32, dikurang dengan ukuran kernel 5 (F), ditambah 0 (tanpa padding), dan dibagi dengan stride 1 (S) dan ditambah 1\n",
    "- $$\\frac{(32 - 5 + 2*0)}{1} + 1 = 28$$\n",
    "- Setelah melewati layer 2, gambar akan berubah menjadi 16 dimensi (RGB) dengan ukuran 14x14 piksel. Mengapa menjadi 14 piksel?\n",
    "- Karena layer setelahnya adalah pooling layer dengan ukuran 2x2 dan stride 2 sehingga ukuran gambar dikurangi menjadi setengahnya\n",
    "- Setelah melewati layer 3, gambar akan berubah menjadi 16 channel dengan ukuran 10x10 piksel.\n",
    "- Dan setelah melewati layer ke 4, gambar akan memiliki ukuran 16 channel dengan ukuran 5x5 piksel. Karena layer setelahnya adalah pooling layer dengan ukuran 2x2 dan stride 2 sehingga ukuran gambar dikurangi menjadi setengahnya\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ConvNet().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Mendefinisikan fungsi loss dan optimizer**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tahap Training dan Testing\n",
    "**Training Loop**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [100/12500], Loss: 2.3435\n",
      "Epoch [1/4], Step [200/12500], Loss: 2.3089\n",
      "Epoch [1/4], Step [300/12500], Loss: 2.2973\n",
      "Epoch [1/4], Step [400/12500], Loss: 2.2694\n",
      "Epoch [1/4], Step [500/12500], Loss: 2.2626\n",
      "Epoch [1/4], Step [600/12500], Loss: 2.3437\n",
      "Epoch [1/4], Step [700/12500], Loss: 2.3100\n",
      "Epoch [1/4], Step [800/12500], Loss: 2.2801\n",
      "Epoch [1/4], Step [900/12500], Loss: 2.3004\n",
      "Epoch [1/4], Step [1000/12500], Loss: 2.3195\n",
      "Epoch [1/4], Step [1100/12500], Loss: 2.3014\n",
      "Epoch [1/4], Step [1200/12500], Loss: 2.3000\n",
      "Epoch [1/4], Step [1300/12500], Loss: 2.2890\n",
      "Epoch [1/4], Step [1400/12500], Loss: 2.3101\n",
      "Epoch [1/4], Step [1500/12500], Loss: 2.2841\n",
      "Epoch [1/4], Step [1600/12500], Loss: 2.2801\n",
      "Epoch [1/4], Step [1700/12500], Loss: 2.3314\n",
      "Epoch [1/4], Step [1800/12500], Loss: 2.3013\n",
      "Epoch [1/4], Step [1900/12500], Loss: 2.3246\n",
      "Epoch [1/4], Step [2000/12500], Loss: 2.3242\n",
      "Epoch [1/4], Step [2100/12500], Loss: 2.3208\n",
      "Epoch [1/4], Step [2200/12500], Loss: 2.3092\n",
      "Epoch [1/4], Step [2300/12500], Loss: 2.2979\n",
      "Epoch [1/4], Step [2400/12500], Loss: 2.2908\n",
      "Epoch [1/4], Step [2500/12500], Loss: 2.3146\n",
      "Epoch [1/4], Step [2600/12500], Loss: 2.3217\n",
      "Epoch [1/4], Step [2700/12500], Loss: 2.3003\n",
      "Epoch [1/4], Step [2800/12500], Loss: 2.2791\n",
      "Epoch [1/4], Step [2900/12500], Loss: 2.2910\n",
      "Epoch [1/4], Step [3000/12500], Loss: 2.3203\n",
      "Epoch [1/4], Step [3100/12500], Loss: 2.3285\n",
      "Epoch [1/4], Step [3200/12500], Loss: 2.3158\n",
      "Epoch [1/4], Step [3300/12500], Loss: 2.2959\n",
      "Epoch [1/4], Step [3400/12500], Loss: 2.3163\n",
      "Epoch [1/4], Step [3500/12500], Loss: 2.3368\n",
      "Epoch [1/4], Step [3600/12500], Loss: 2.3084\n",
      "Epoch [1/4], Step [3700/12500], Loss: 2.3046\n",
      "Epoch [1/4], Step [3800/12500], Loss: 2.2987\n",
      "Epoch [1/4], Step [3900/12500], Loss: 2.2964\n",
      "Epoch [1/4], Step [4000/12500], Loss: 2.3195\n",
      "Epoch [1/4], Step [4100/12500], Loss: 2.2931\n",
      "Epoch [1/4], Step [4200/12500], Loss: 2.2990\n",
      "Epoch [1/4], Step [4300/12500], Loss: 2.3131\n",
      "Epoch [1/4], Step [4400/12500], Loss: 2.2890\n",
      "Epoch [1/4], Step [4500/12500], Loss: 2.3003\n",
      "Epoch [1/4], Step [4600/12500], Loss: 2.2998\n",
      "Epoch [1/4], Step [4700/12500], Loss: 2.2862\n",
      "Epoch [1/4], Step [4800/12500], Loss: 2.3141\n",
      "Epoch [1/4], Step [4900/12500], Loss: 2.3140\n",
      "Epoch [1/4], Step [5000/12500], Loss: 2.3085\n",
      "Epoch [1/4], Step [5100/12500], Loss: 2.3178\n",
      "Epoch [1/4], Step [5200/12500], Loss: 2.3021\n",
      "Epoch [1/4], Step [5300/12500], Loss: 2.3169\n",
      "Epoch [1/4], Step [5400/12500], Loss: 2.2930\n",
      "Epoch [1/4], Step [5500/12500], Loss: 2.2847\n",
      "Epoch [1/4], Step [5600/12500], Loss: 2.3248\n",
      "Epoch [1/4], Step [5700/12500], Loss: 2.3205\n",
      "Epoch [1/4], Step [5800/12500], Loss: 2.2923\n",
      "Epoch [1/4], Step [5900/12500], Loss: 2.2900\n",
      "Epoch [1/4], Step [6000/12500], Loss: 2.2810\n",
      "Epoch [1/4], Step [6100/12500], Loss: 2.2987\n",
      "Epoch [1/4], Step [6200/12500], Loss: 2.3002\n",
      "Epoch [1/4], Step [6300/12500], Loss: 2.2952\n",
      "Epoch [1/4], Step [6400/12500], Loss: 2.2832\n",
      "Epoch [1/4], Step [6500/12500], Loss: 2.3107\n",
      "Epoch [1/4], Step [6600/12500], Loss: 2.2877\n",
      "Epoch [1/4], Step [6700/12500], Loss: 2.3064\n",
      "Epoch [1/4], Step [6800/12500], Loss: 2.3020\n",
      "Epoch [1/4], Step [6900/12500], Loss: 2.2845\n",
      "Epoch [1/4], Step [7000/12500], Loss: 2.2901\n",
      "Epoch [1/4], Step [7100/12500], Loss: 2.3103\n",
      "Epoch [1/4], Step [7200/12500], Loss: 2.2980\n",
      "Epoch [1/4], Step [7300/12500], Loss: 2.2942\n",
      "Epoch [1/4], Step [7400/12500], Loss: 2.2987\n",
      "Epoch [1/4], Step [7500/12500], Loss: 2.2968\n",
      "Epoch [1/4], Step [7600/12500], Loss: 2.3221\n",
      "Epoch [1/4], Step [7700/12500], Loss: 2.3004\n",
      "Epoch [1/4], Step [7800/12500], Loss: 2.3001\n",
      "Epoch [1/4], Step [7900/12500], Loss: 2.2964\n",
      "Epoch [1/4], Step [8000/12500], Loss: 2.2931\n",
      "Epoch [1/4], Step [8100/12500], Loss: 2.2992\n",
      "Epoch [1/4], Step [8200/12500], Loss: 2.2912\n",
      "Epoch [1/4], Step [8300/12500], Loss: 2.2976\n",
      "Epoch [1/4], Step [8400/12500], Loss: 2.2782\n",
      "Epoch [1/4], Step [8500/12500], Loss: 2.2899\n",
      "Epoch [1/4], Step [8600/12500], Loss: 2.2960\n",
      "Epoch [1/4], Step [8700/12500], Loss: 2.3062\n",
      "Epoch [1/4], Step [8800/12500], Loss: 2.3117\n",
      "Epoch [1/4], Step [8900/12500], Loss: 2.2917\n",
      "Epoch [1/4], Step [9000/12500], Loss: 2.2728\n",
      "Epoch [1/4], Step [9100/12500], Loss: 2.2909\n",
      "Epoch [1/4], Step [9200/12500], Loss: 2.3003\n",
      "Epoch [1/4], Step [9300/12500], Loss: 2.2824\n",
      "Epoch [1/4], Step [9400/12500], Loss: 2.2997\n",
      "Epoch [1/4], Step [9500/12500], Loss: 2.2748\n",
      "Epoch [1/4], Step [9600/12500], Loss: 2.2674\n",
      "Epoch [1/4], Step [9700/12500], Loss: 2.2931\n",
      "Epoch [1/4], Step [9800/12500], Loss: 2.2834\n",
      "Epoch [1/4], Step [9900/12500], Loss: 2.2822\n",
      "Epoch [1/4], Step [10000/12500], Loss: 2.2865\n",
      "Epoch [1/4], Step [10100/12500], Loss: 2.3046\n",
      "Epoch [1/4], Step [10200/12500], Loss: 2.2840\n",
      "Epoch [1/4], Step [10300/12500], Loss: 2.2781\n",
      "Epoch [1/4], Step [10400/12500], Loss: 2.2897\n",
      "Epoch [1/4], Step [10500/12500], Loss: 2.2973\n",
      "Epoch [1/4], Step [10600/12500], Loss: 2.2852\n",
      "Epoch [1/4], Step [10700/12500], Loss: 2.3170\n",
      "Epoch [1/4], Step [10800/12500], Loss: 2.2658\n",
      "Epoch [1/4], Step [10900/12500], Loss: 2.3234\n",
      "Epoch [1/4], Step [11000/12500], Loss: 2.2974\n",
      "Epoch [1/4], Step [11100/12500], Loss: 2.2681\n",
      "Epoch [1/4], Step [11200/12500], Loss: 2.3106\n",
      "Epoch [1/4], Step [11300/12500], Loss: 2.2999\n",
      "Epoch [1/4], Step [11400/12500], Loss: 2.2871\n",
      "Epoch [1/4], Step [11500/12500], Loss: 2.2412\n",
      "Epoch [1/4], Step [11600/12500], Loss: 2.2893\n",
      "Epoch [1/4], Step [11700/12500], Loss: 2.3013\n",
      "Epoch [1/4], Step [11800/12500], Loss: 2.2749\n",
      "Epoch [1/4], Step [11900/12500], Loss: 2.2435\n",
      "Epoch [1/4], Step [12000/12500], Loss: 2.2587\n",
      "Epoch [1/4], Step [12100/12500], Loss: 2.2698\n",
      "Epoch [1/4], Step [12200/12500], Loss: 2.2574\n",
      "Epoch [1/4], Step [12300/12500], Loss: 2.3176\n",
      "Epoch [1/4], Step [12400/12500], Loss: 2.2907\n",
      "Epoch [1/4], Step [12500/12500], Loss: 2.2983\n",
      "Epoch [2/4], Step [100/12500], Loss: 2.2550\n",
      "Epoch [2/4], Step [200/12500], Loss: 2.3070\n",
      "Epoch [2/4], Step [300/12500], Loss: 2.2504\n",
      "Epoch [2/4], Step [400/12500], Loss: 2.2700\n",
      "Epoch [2/4], Step [500/12500], Loss: 2.2561\n",
      "Epoch [2/4], Step [600/12500], Loss: 2.3106\n",
      "Epoch [2/4], Step [700/12500], Loss: 2.2242\n",
      "Epoch [2/4], Step [800/12500], Loss: 2.2472\n",
      "Epoch [2/4], Step [900/12500], Loss: 2.2267\n",
      "Epoch [2/4], Step [1000/12500], Loss: 2.3113\n",
      "Epoch [2/4], Step [1100/12500], Loss: 2.3265\n",
      "Epoch [2/4], Step [1200/12500], Loss: 2.3448\n",
      "Epoch [2/4], Step [1300/12500], Loss: 2.1979\n",
      "Epoch [2/4], Step [1400/12500], Loss: 2.2198\n",
      "Epoch [2/4], Step [1500/12500], Loss: 2.2078\n",
      "Epoch [2/4], Step [1600/12500], Loss: 2.0801\n",
      "Epoch [2/4], Step [1700/12500], Loss: 2.2164\n",
      "Epoch [2/4], Step [1800/12500], Loss: 2.2782\n",
      "Epoch [2/4], Step [1900/12500], Loss: 2.3183\n",
      "Epoch [2/4], Step [2000/12500], Loss: 2.1828\n",
      "Epoch [2/4], Step [2100/12500], Loss: 2.2361\n",
      "Epoch [2/4], Step [2200/12500], Loss: 2.0492\n",
      "Epoch [2/4], Step [2300/12500], Loss: 2.1439\n",
      "Epoch [2/4], Step [2400/12500], Loss: 2.1469\n",
      "Epoch [2/4], Step [2500/12500], Loss: 2.1282\n",
      "Epoch [2/4], Step [2600/12500], Loss: 2.0132\n",
      "Epoch [2/4], Step [2700/12500], Loss: 2.0466\n",
      "Epoch [2/4], Step [2800/12500], Loss: 2.1271\n",
      "Epoch [2/4], Step [2900/12500], Loss: 1.8607\n",
      "Epoch [2/4], Step [3000/12500], Loss: 2.0422\n",
      "Epoch [2/4], Step [3100/12500], Loss: 2.1083\n",
      "Epoch [2/4], Step [3200/12500], Loss: 2.3021\n",
      "Epoch [2/4], Step [3300/12500], Loss: 2.4645\n",
      "Epoch [2/4], Step [3400/12500], Loss: 1.9530\n",
      "Epoch [2/4], Step [3500/12500], Loss: 1.9518\n",
      "Epoch [2/4], Step [3600/12500], Loss: 2.3925\n",
      "Epoch [2/4], Step [3700/12500], Loss: 2.1245\n",
      "Epoch [2/4], Step [3800/12500], Loss: 2.1709\n",
      "Epoch [2/4], Step [3900/12500], Loss: 1.9267\n",
      "Epoch [2/4], Step [4000/12500], Loss: 2.0249\n",
      "Epoch [2/4], Step [4100/12500], Loss: 2.1339\n",
      "Epoch [2/4], Step [4200/12500], Loss: 1.5909\n",
      "Epoch [2/4], Step [4300/12500], Loss: 1.9182\n",
      "Epoch [2/4], Step [4400/12500], Loss: 1.4396\n",
      "Epoch [2/4], Step [4500/12500], Loss: 2.0689\n",
      "Epoch [2/4], Step [4600/12500], Loss: 2.2348\n",
      "Epoch [2/4], Step [4700/12500], Loss: 1.5737\n",
      "Epoch [2/4], Step [4800/12500], Loss: 2.5251\n",
      "Epoch [2/4], Step [4900/12500], Loss: 2.3340\n",
      "Epoch [2/4], Step [5000/12500], Loss: 1.9085\n",
      "Epoch [2/4], Step [5100/12500], Loss: 1.6612\n",
      "Epoch [2/4], Step [5200/12500], Loss: 1.9430\n",
      "Epoch [2/4], Step [5300/12500], Loss: 1.7360\n",
      "Epoch [2/4], Step [5400/12500], Loss: 2.4966\n",
      "Epoch [2/4], Step [5500/12500], Loss: 1.9348\n",
      "Epoch [2/4], Step [5600/12500], Loss: 2.2256\n",
      "Epoch [2/4], Step [5700/12500], Loss: 2.2936\n",
      "Epoch [2/4], Step [5800/12500], Loss: 2.0278\n",
      "Epoch [2/4], Step [5900/12500], Loss: 1.8682\n",
      "Epoch [2/4], Step [6000/12500], Loss: 1.9996\n",
      "Epoch [2/4], Step [6100/12500], Loss: 2.3501\n",
      "Epoch [2/4], Step [6200/12500], Loss: 2.6931\n",
      "Epoch [2/4], Step [6300/12500], Loss: 1.7433\n",
      "Epoch [2/4], Step [6400/12500], Loss: 2.0011\n",
      "Epoch [2/4], Step [6500/12500], Loss: 1.9500\n",
      "Epoch [2/4], Step [6600/12500], Loss: 1.6550\n",
      "Epoch [2/4], Step [6700/12500], Loss: 1.9275\n",
      "Epoch [2/4], Step [6800/12500], Loss: 1.6534\n",
      "Epoch [2/4], Step [6900/12500], Loss: 1.6005\n",
      "Epoch [2/4], Step [7000/12500], Loss: 1.7938\n",
      "Epoch [2/4], Step [7100/12500], Loss: 1.3515\n",
      "Epoch [2/4], Step [7200/12500], Loss: 2.7734\n",
      "Epoch [2/4], Step [7300/12500], Loss: 2.0587\n",
      "Epoch [2/4], Step [7400/12500], Loss: 1.9452\n",
      "Epoch [2/4], Step [7500/12500], Loss: 1.7507\n",
      "Epoch [2/4], Step [7600/12500], Loss: 2.1470\n",
      "Epoch [2/4], Step [7700/12500], Loss: 1.8326\n",
      "Epoch [2/4], Step [7800/12500], Loss: 1.5311\n",
      "Epoch [2/4], Step [7900/12500], Loss: 2.0789\n",
      "Epoch [2/4], Step [8000/12500], Loss: 2.1237\n",
      "Epoch [2/4], Step [8100/12500], Loss: 2.2444\n",
      "Epoch [2/4], Step [8200/12500], Loss: 1.7784\n",
      "Epoch [2/4], Step [8300/12500], Loss: 1.9730\n",
      "Epoch [2/4], Step [8400/12500], Loss: 1.9787\n",
      "Epoch [2/4], Step [8500/12500], Loss: 1.7894\n",
      "Epoch [2/4], Step [8600/12500], Loss: 2.4550\n",
      "Epoch [2/4], Step [8700/12500], Loss: 1.7271\n",
      "Epoch [2/4], Step [8800/12500], Loss: 1.8156\n",
      "Epoch [2/4], Step [8900/12500], Loss: 1.5019\n",
      "Epoch [2/4], Step [9000/12500], Loss: 2.4307\n",
      "Epoch [2/4], Step [9100/12500], Loss: 1.7161\n",
      "Epoch [2/4], Step [9200/12500], Loss: 1.3896\n",
      "Epoch [2/4], Step [9300/12500], Loss: 2.1592\n",
      "Epoch [2/4], Step [9400/12500], Loss: 2.4324\n",
      "Epoch [2/4], Step [9500/12500], Loss: 2.3408\n",
      "Epoch [2/4], Step [9600/12500], Loss: 2.0936\n",
      "Epoch [2/4], Step [9700/12500], Loss: 1.7450\n",
      "Epoch [2/4], Step [9800/12500], Loss: 1.7127\n",
      "Epoch [2/4], Step [9900/12500], Loss: 2.3495\n",
      "Epoch [2/4], Step [10000/12500], Loss: 1.7101\n",
      "Epoch [2/4], Step [10100/12500], Loss: 1.6872\n",
      "Epoch [2/4], Step [10200/12500], Loss: 1.7185\n",
      "Epoch [2/4], Step [10300/12500], Loss: 2.5049\n",
      "Epoch [2/4], Step [10400/12500], Loss: 2.1665\n",
      "Epoch [2/4], Step [10500/12500], Loss: 1.9507\n",
      "Epoch [2/4], Step [10600/12500], Loss: 1.5576\n",
      "Epoch [2/4], Step [10700/12500], Loss: 1.4437\n",
      "Epoch [2/4], Step [10800/12500], Loss: 1.7300\n",
      "Epoch [2/4], Step [10900/12500], Loss: 2.6409\n",
      "Epoch [2/4], Step [11000/12500], Loss: 1.6511\n",
      "Epoch [2/4], Step [11100/12500], Loss: 2.6987\n",
      "Epoch [2/4], Step [11200/12500], Loss: 2.9569\n",
      "Epoch [2/4], Step [11300/12500], Loss: 2.0067\n",
      "Epoch [2/4], Step [11400/12500], Loss: 1.7662\n",
      "Epoch [2/4], Step [11500/12500], Loss: 1.7617\n",
      "Epoch [2/4], Step [11600/12500], Loss: 2.4414\n",
      "Epoch [2/4], Step [11700/12500], Loss: 1.9284\n",
      "Epoch [2/4], Step [11800/12500], Loss: 2.0992\n",
      "Epoch [2/4], Step [11900/12500], Loss: 1.2281\n",
      "Epoch [2/4], Step [12000/12500], Loss: 1.8962\n",
      "Epoch [2/4], Step [12100/12500], Loss: 2.0129\n",
      "Epoch [2/4], Step [12200/12500], Loss: 1.7050\n",
      "Epoch [2/4], Step [12300/12500], Loss: 2.1942\n",
      "Epoch [2/4], Step [12400/12500], Loss: 1.8710\n",
      "Epoch [2/4], Step [12500/12500], Loss: 2.0722\n",
      "Epoch [3/4], Step [100/12500], Loss: 1.7729\n",
      "Epoch [3/4], Step [200/12500], Loss: 1.7564\n",
      "Epoch [3/4], Step [300/12500], Loss: 2.0813\n",
      "Epoch [3/4], Step [400/12500], Loss: 1.7292\n",
      "Epoch [3/4], Step [500/12500], Loss: 2.4470\n",
      "Epoch [3/4], Step [600/12500], Loss: 2.2389\n",
      "Epoch [3/4], Step [700/12500], Loss: 1.6896\n",
      "Epoch [3/4], Step [800/12500], Loss: 1.9458\n",
      "Epoch [3/4], Step [900/12500], Loss: 1.8046\n",
      "Epoch [3/4], Step [1000/12500], Loss: 2.1013\n",
      "Epoch [3/4], Step [1100/12500], Loss: 1.7170\n",
      "Epoch [3/4], Step [1200/12500], Loss: 1.8694\n",
      "Epoch [3/4], Step [1300/12500], Loss: 1.4134\n",
      "Epoch [3/4], Step [1400/12500], Loss: 1.6534\n",
      "Epoch [3/4], Step [1500/12500], Loss: 1.3428\n",
      "Epoch [3/4], Step [1600/12500], Loss: 1.2398\n",
      "Epoch [3/4], Step [1700/12500], Loss: 1.3484\n",
      "Epoch [3/4], Step [1800/12500], Loss: 1.7514\n",
      "Epoch [3/4], Step [1900/12500], Loss: 2.2357\n",
      "Epoch [3/4], Step [2000/12500], Loss: 1.5048\n",
      "Epoch [3/4], Step [2100/12500], Loss: 1.6046\n",
      "Epoch [3/4], Step [2200/12500], Loss: 2.3520\n",
      "Epoch [3/4], Step [2300/12500], Loss: 1.4404\n",
      "Epoch [3/4], Step [2400/12500], Loss: 1.6411\n",
      "Epoch [3/4], Step [2500/12500], Loss: 3.2041\n",
      "Epoch [3/4], Step [2600/12500], Loss: 2.3936\n",
      "Epoch [3/4], Step [2700/12500], Loss: 2.4025\n",
      "Epoch [3/4], Step [2800/12500], Loss: 1.9502\n",
      "Epoch [3/4], Step [2900/12500], Loss: 1.5214\n",
      "Epoch [3/4], Step [3000/12500], Loss: 1.9427\n",
      "Epoch [3/4], Step [3100/12500], Loss: 2.1501\n",
      "Epoch [3/4], Step [3200/12500], Loss: 1.4450\n",
      "Epoch [3/4], Step [3300/12500], Loss: 1.6083\n",
      "Epoch [3/4], Step [3400/12500], Loss: 1.8608\n",
      "Epoch [3/4], Step [3500/12500], Loss: 2.2251\n",
      "Epoch [3/4], Step [3600/12500], Loss: 1.2793\n",
      "Epoch [3/4], Step [3700/12500], Loss: 1.8881\n",
      "Epoch [3/4], Step [3800/12500], Loss: 1.5886\n",
      "Epoch [3/4], Step [3900/12500], Loss: 1.3440\n",
      "Epoch [3/4], Step [4000/12500], Loss: 1.8044\n",
      "Epoch [3/4], Step [4100/12500], Loss: 2.1205\n",
      "Epoch [3/4], Step [4200/12500], Loss: 1.5782\n",
      "Epoch [3/4], Step [4300/12500], Loss: 1.5145\n",
      "Epoch [3/4], Step [4400/12500], Loss: 1.5491\n",
      "Epoch [3/4], Step [4500/12500], Loss: 1.4694\n",
      "Epoch [3/4], Step [4600/12500], Loss: 2.7831\n",
      "Epoch [3/4], Step [4700/12500], Loss: 1.7772\n",
      "Epoch [3/4], Step [4800/12500], Loss: 1.7482\n",
      "Epoch [3/4], Step [4900/12500], Loss: 2.0887\n",
      "Epoch [3/4], Step [5000/12500], Loss: 2.1437\n",
      "Epoch [3/4], Step [5100/12500], Loss: 1.5333\n",
      "Epoch [3/4], Step [5200/12500], Loss: 1.5418\n",
      "Epoch [3/4], Step [5300/12500], Loss: 1.3271\n",
      "Epoch [3/4], Step [5400/12500], Loss: 1.7550\n",
      "Epoch [3/4], Step [5500/12500], Loss: 2.1580\n",
      "Epoch [3/4], Step [5600/12500], Loss: 2.0181\n",
      "Epoch [3/4], Step [5700/12500], Loss: 1.6331\n",
      "Epoch [3/4], Step [5800/12500], Loss: 1.4648\n",
      "Epoch [3/4], Step [5900/12500], Loss: 1.4315\n",
      "Epoch [3/4], Step [6000/12500], Loss: 2.0340\n",
      "Epoch [3/4], Step [6100/12500], Loss: 2.9506\n",
      "Epoch [3/4], Step [6200/12500], Loss: 1.4267\n",
      "Epoch [3/4], Step [6300/12500], Loss: 1.6349\n",
      "Epoch [3/4], Step [6400/12500], Loss: 2.2130\n",
      "Epoch [3/4], Step [6500/12500], Loss: 1.6175\n",
      "Epoch [3/4], Step [6600/12500], Loss: 1.9645\n",
      "Epoch [3/4], Step [6700/12500], Loss: 1.1642\n",
      "Epoch [3/4], Step [6800/12500], Loss: 1.5737\n",
      "Epoch [3/4], Step [6900/12500], Loss: 1.1765\n",
      "Epoch [3/4], Step [7000/12500], Loss: 2.1858\n",
      "Epoch [3/4], Step [7100/12500], Loss: 1.6252\n",
      "Epoch [3/4], Step [7200/12500], Loss: 1.5690\n",
      "Epoch [3/4], Step [7300/12500], Loss: 2.4121\n",
      "Epoch [3/4], Step [7400/12500], Loss: 1.5205\n",
      "Epoch [3/4], Step [7500/12500], Loss: 1.5259\n",
      "Epoch [3/4], Step [7600/12500], Loss: 1.1493\n",
      "Epoch [3/4], Step [7700/12500], Loss: 2.9889\n",
      "Epoch [3/4], Step [7800/12500], Loss: 1.5557\n",
      "Epoch [3/4], Step [7900/12500], Loss: 1.2859\n",
      "Epoch [3/4], Step [8000/12500], Loss: 2.1407\n",
      "Epoch [3/4], Step [8100/12500], Loss: 2.4309\n",
      "Epoch [3/4], Step [8200/12500], Loss: 1.0956\n",
      "Epoch [3/4], Step [8300/12500], Loss: 1.5887\n",
      "Epoch [3/4], Step [8400/12500], Loss: 1.8716\n",
      "Epoch [3/4], Step [8500/12500], Loss: 1.2262\n",
      "Epoch [3/4], Step [8600/12500], Loss: 2.3508\n",
      "Epoch [3/4], Step [8700/12500], Loss: 1.5713\n",
      "Epoch [3/4], Step [8800/12500], Loss: 1.2106\n",
      "Epoch [3/4], Step [8900/12500], Loss: 2.8989\n",
      "Epoch [3/4], Step [9000/12500], Loss: 1.7625\n",
      "Epoch [3/4], Step [9100/12500], Loss: 1.6873\n",
      "Epoch [3/4], Step [9200/12500], Loss: 1.6309\n",
      "Epoch [3/4], Step [9300/12500], Loss: 1.9964\n",
      "Epoch [3/4], Step [9400/12500], Loss: 1.7835\n",
      "Epoch [3/4], Step [9500/12500], Loss: 0.9997\n",
      "Epoch [3/4], Step [9600/12500], Loss: 1.5433\n",
      "Epoch [3/4], Step [9700/12500], Loss: 1.5540\n",
      "Epoch [3/4], Step [9800/12500], Loss: 1.1418\n",
      "Epoch [3/4], Step [9900/12500], Loss: 1.8290\n",
      "Epoch [3/4], Step [10000/12500], Loss: 1.6784\n",
      "Epoch [3/4], Step [10100/12500], Loss: 1.5567\n",
      "Epoch [3/4], Step [10200/12500], Loss: 2.0653\n",
      "Epoch [3/4], Step [10300/12500], Loss: 1.5392\n",
      "Epoch [3/4], Step [10400/12500], Loss: 1.4036\n",
      "Epoch [3/4], Step [10500/12500], Loss: 2.3758\n",
      "Epoch [3/4], Step [10600/12500], Loss: 1.6176\n",
      "Epoch [3/4], Step [10700/12500], Loss: 1.6298\n",
      "Epoch [3/4], Step [10800/12500], Loss: 1.7999\n",
      "Epoch [3/4], Step [10900/12500], Loss: 1.7043\n",
      "Epoch [3/4], Step [11000/12500], Loss: 1.9695\n",
      "Epoch [3/4], Step [11100/12500], Loss: 1.7437\n",
      "Epoch [3/4], Step [11200/12500], Loss: 1.4810\n",
      "Epoch [3/4], Step [11300/12500], Loss: 0.8870\n",
      "Epoch [3/4], Step [11400/12500], Loss: 2.3044\n",
      "Epoch [3/4], Step [11500/12500], Loss: 1.5068\n",
      "Epoch [3/4], Step [11600/12500], Loss: 1.8052\n",
      "Epoch [3/4], Step [11700/12500], Loss: 1.7153\n",
      "Epoch [3/4], Step [11800/12500], Loss: 1.6396\n",
      "Epoch [3/4], Step [11900/12500], Loss: 1.1072\n",
      "Epoch [3/4], Step [12000/12500], Loss: 1.5335\n",
      "Epoch [3/4], Step [12100/12500], Loss: 2.1288\n",
      "Epoch [3/4], Step [12200/12500], Loss: 2.0427\n",
      "Epoch [3/4], Step [12300/12500], Loss: 1.9037\n",
      "Epoch [3/4], Step [12400/12500], Loss: 2.0288\n",
      "Epoch [3/4], Step [12500/12500], Loss: 1.6096\n",
      "Epoch [4/4], Step [100/12500], Loss: 1.4739\n",
      "Epoch [4/4], Step [200/12500], Loss: 1.5021\n",
      "Epoch [4/4], Step [300/12500], Loss: 0.8332\n",
      "Epoch [4/4], Step [400/12500], Loss: 1.7118\n",
      "Epoch [4/4], Step [500/12500], Loss: 1.1898\n",
      "Epoch [4/4], Step [600/12500], Loss: 1.9040\n",
      "Epoch [4/4], Step [700/12500], Loss: 2.2546\n",
      "Epoch [4/4], Step [800/12500], Loss: 1.9426\n",
      "Epoch [4/4], Step [900/12500], Loss: 1.5407\n",
      "Epoch [4/4], Step [1000/12500], Loss: 1.5106\n",
      "Epoch [4/4], Step [1100/12500], Loss: 1.3921\n",
      "Epoch [4/4], Step [1200/12500], Loss: 1.4556\n",
      "Epoch [4/4], Step [1300/12500], Loss: 1.7926\n",
      "Epoch [4/4], Step [1400/12500], Loss: 1.2330\n",
      "Epoch [4/4], Step [1500/12500], Loss: 1.1739\n",
      "Epoch [4/4], Step [1600/12500], Loss: 1.8329\n",
      "Epoch [4/4], Step [1700/12500], Loss: 1.3820\n",
      "Epoch [4/4], Step [1800/12500], Loss: 1.4453\n",
      "Epoch [4/4], Step [1900/12500], Loss: 1.8584\n",
      "Epoch [4/4], Step [2000/12500], Loss: 1.8603\n",
      "Epoch [4/4], Step [2100/12500], Loss: 0.7666\n",
      "Epoch [4/4], Step [2200/12500], Loss: 1.3311\n",
      "Epoch [4/4], Step [2300/12500], Loss: 1.4328\n",
      "Epoch [4/4], Step [2400/12500], Loss: 1.3601\n",
      "Epoch [4/4], Step [2500/12500], Loss: 2.0705\n",
      "Epoch [4/4], Step [2600/12500], Loss: 1.4641\n",
      "Epoch [4/4], Step [2700/12500], Loss: 1.0539\n",
      "Epoch [4/4], Step [2800/12500], Loss: 1.2689\n",
      "Epoch [4/4], Step [2900/12500], Loss: 1.0392\n",
      "Epoch [4/4], Step [3000/12500], Loss: 1.2533\n",
      "Epoch [4/4], Step [3100/12500], Loss: 1.4687\n",
      "Epoch [4/4], Step [3200/12500], Loss: 2.2640\n",
      "Epoch [4/4], Step [3300/12500], Loss: 1.7688\n",
      "Epoch [4/4], Step [3400/12500], Loss: 1.0361\n",
      "Epoch [4/4], Step [3500/12500], Loss: 1.3130\n",
      "Epoch [4/4], Step [3600/12500], Loss: 1.7865\n",
      "Epoch [4/4], Step [3700/12500], Loss: 1.9068\n",
      "Epoch [4/4], Step [3800/12500], Loss: 2.5591\n",
      "Epoch [4/4], Step [3900/12500], Loss: 1.1805\n",
      "Epoch [4/4], Step [4000/12500], Loss: 1.5302\n",
      "Epoch [4/4], Step [4100/12500], Loss: 1.5481\n",
      "Epoch [4/4], Step [4200/12500], Loss: 2.0198\n",
      "Epoch [4/4], Step [4300/12500], Loss: 2.0268\n",
      "Epoch [4/4], Step [4400/12500], Loss: 1.2384\n",
      "Epoch [4/4], Step [4500/12500], Loss: 1.9143\n",
      "Epoch [4/4], Step [4600/12500], Loss: 1.5617\n",
      "Epoch [4/4], Step [4700/12500], Loss: 1.8866\n",
      "Epoch [4/4], Step [4800/12500], Loss: 1.2804\n",
      "Epoch [4/4], Step [4900/12500], Loss: 1.9839\n",
      "Epoch [4/4], Step [5000/12500], Loss: 1.1126\n",
      "Epoch [4/4], Step [5100/12500], Loss: 1.0782\n",
      "Epoch [4/4], Step [5200/12500], Loss: 1.5838\n",
      "Epoch [4/4], Step [5300/12500], Loss: 1.7210\n",
      "Epoch [4/4], Step [5400/12500], Loss: 1.2488\n",
      "Epoch [4/4], Step [5500/12500], Loss: 2.5319\n",
      "Epoch [4/4], Step [5600/12500], Loss: 1.3814\n",
      "Epoch [4/4], Step [5700/12500], Loss: 1.6974\n",
      "Epoch [4/4], Step [5800/12500], Loss: 1.8039\n",
      "Epoch [4/4], Step [5900/12500], Loss: 1.8024\n",
      "Epoch [4/4], Step [6000/12500], Loss: 1.1126\n",
      "Epoch [4/4], Step [6100/12500], Loss: 1.3317\n",
      "Epoch [4/4], Step [6200/12500], Loss: 1.4876\n",
      "Epoch [4/4], Step [6300/12500], Loss: 2.1837\n",
      "Epoch [4/4], Step [6400/12500], Loss: 2.1966\n",
      "Epoch [4/4], Step [6500/12500], Loss: 1.4652\n",
      "Epoch [4/4], Step [6600/12500], Loss: 2.0811\n",
      "Epoch [4/4], Step [6700/12500], Loss: 1.5533\n",
      "Epoch [4/4], Step [6800/12500], Loss: 1.2489\n",
      "Epoch [4/4], Step [6900/12500], Loss: 1.5013\n",
      "Epoch [4/4], Step [7000/12500], Loss: 1.4459\n",
      "Epoch [4/4], Step [7100/12500], Loss: 1.4835\n",
      "Epoch [4/4], Step [7200/12500], Loss: 1.2316\n",
      "Epoch [4/4], Step [7300/12500], Loss: 0.4602\n",
      "Epoch [4/4], Step [7400/12500], Loss: 1.3031\n",
      "Epoch [4/4], Step [7500/12500], Loss: 1.4237\n",
      "Epoch [4/4], Step [7600/12500], Loss: 1.2719\n",
      "Epoch [4/4], Step [7700/12500], Loss: 1.4488\n",
      "Epoch [4/4], Step [7800/12500], Loss: 1.8798\n",
      "Epoch [4/4], Step [7900/12500], Loss: 1.9563\n",
      "Epoch [4/4], Step [8000/12500], Loss: 1.1004\n",
      "Epoch [4/4], Step [8100/12500], Loss: 1.1337\n",
      "Epoch [4/4], Step [8200/12500], Loss: 1.1560\n",
      "Epoch [4/4], Step [8300/12500], Loss: 1.6114\n",
      "Epoch [4/4], Step [8400/12500], Loss: 1.4136\n",
      "Epoch [4/4], Step [8500/12500], Loss: 1.5414\n",
      "Epoch [4/4], Step [8600/12500], Loss: 1.4141\n",
      "Epoch [4/4], Step [8700/12500], Loss: 1.6365\n",
      "Epoch [4/4], Step [8800/12500], Loss: 1.0576\n",
      "Epoch [4/4], Step [8900/12500], Loss: 2.1380\n",
      "Epoch [4/4], Step [9000/12500], Loss: 1.7361\n",
      "Epoch [4/4], Step [9100/12500], Loss: 1.9644\n",
      "Epoch [4/4], Step [9200/12500], Loss: 1.5986\n",
      "Epoch [4/4], Step [9300/12500], Loss: 1.8969\n",
      "Epoch [4/4], Step [9400/12500], Loss: 1.3256\n",
      "Epoch [4/4], Step [9500/12500], Loss: 1.5087\n",
      "Epoch [4/4], Step [9600/12500], Loss: 2.3815\n",
      "Epoch [4/4], Step [9700/12500], Loss: 0.8642\n",
      "Epoch [4/4], Step [9800/12500], Loss: 1.8185\n",
      "Epoch [4/4], Step [9900/12500], Loss: 1.6779\n",
      "Epoch [4/4], Step [10000/12500], Loss: 1.4692\n",
      "Epoch [4/4], Step [10100/12500], Loss: 1.3681\n",
      "Epoch [4/4], Step [10200/12500], Loss: 1.6273\n",
      "Epoch [4/4], Step [10300/12500], Loss: 1.3149\n",
      "Epoch [4/4], Step [10400/12500], Loss: 1.1702\n",
      "Epoch [4/4], Step [10500/12500], Loss: 1.1890\n",
      "Epoch [4/4], Step [10600/12500], Loss: 0.6439\n",
      "Epoch [4/4], Step [10700/12500], Loss: 1.3690\n",
      "Epoch [4/4], Step [10800/12500], Loss: 2.3557\n",
      "Epoch [4/4], Step [10900/12500], Loss: 1.2552\n",
      "Epoch [4/4], Step [11000/12500], Loss: 0.5408\n",
      "Epoch [4/4], Step [11100/12500], Loss: 1.2695\n",
      "Epoch [4/4], Step [11200/12500], Loss: 1.3331\n",
      "Epoch [4/4], Step [11300/12500], Loss: 2.0959\n",
      "Epoch [4/4], Step [11400/12500], Loss: 0.8643\n",
      "Epoch [4/4], Step [11500/12500], Loss: 1.1964\n",
      "Epoch [4/4], Step [11600/12500], Loss: 1.1619\n",
      "Epoch [4/4], Step [11700/12500], Loss: 1.2131\n",
      "Epoch [4/4], Step [11800/12500], Loss: 0.8764\n",
      "Epoch [4/4], Step [11900/12500], Loss: 2.0843\n",
      "Epoch [4/4], Step [12000/12500], Loss: 1.3546\n",
      "Epoch [4/4], Step [12100/12500], Loss: 1.4471\n",
      "Epoch [4/4], Step [12200/12500], Loss: 1.1432\n",
      "Epoch [4/4], Step [12300/12500], Loss: 0.4442\n",
      "Epoch [4/4], Step [12400/12500], Loss: 1.4530\n",
      "Epoch [4/4], Step [12500/12500], Loss: 1.4267\n",
      "Training Selesai\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(trainloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, n_total_steps, loss.item()))\n",
    "\n",
    "print('Training Selesai')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Testing Loop**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.2400%\n",
      "Akurasi kelas- plane: 54.0000%\n",
      "Akurasi kelas- car: 45.3000%\n",
      "Akurasi kelas- bird: 29.8000%\n",
      "Akurasi kelas- cat: 16.2000%\n",
      "Akurasi kelas- deer: 33.1000%\n",
      "Akurasi kelas- dog: 43.0000%\n",
      "Akurasi kelas- frog: 65.1000%\n",
      "Akurasi kelas- horse: 60.9000%\n",
      "Akurasi kelas- ship: 48.2000%\n",
      "Akurasi kelas- truck: 66.8000%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if label == pred:\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    print(f'Accuracy: {n_correct/n_samples*100:.4f}%')\n",
    "\n",
    "    for i in range(10):\n",
    "        print(f'Akurasi kelas- {classes[i]}: {n_class_correct[i]/n_class_samples[i]*100:.4f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}