{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN pada Teks\n",
    "\n",
    "Pada modul sebelumnya, kita telah membahas bagaimana memprediksi data sekuensial. Data dapat berwujud apa saja dan RNN merupakan sebuah bentuk model yang mendukung pemrosesan data sekuensial. Kali ini kita akan mencoba RNN pada bentuk data sekuensial lainnya yaitu teks.\n",
    "\n",
    "### Library yang di Import\n",
    "- `collections` digunakan untuk menyediakan beberapa container datatypes tambahan\n",
    "- `re` digunakan untuk operasi regular expression (regex) pada python\n",
    "- `d2l` digunakan untuk mengimport library yang dibutuhkan dalam modul ini yang bersumber dari buku [d2l.ai](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memuat Dataset\n",
    "\n",
    "Pada modul ini, dataset yang digunakan bersumber dari sebuah buku [The Time Machine, oleh H.G. Wells](http://www.gutenberg.org/ebooks/35). Dataset yang akan digunakan berjumlah 30000 kata dan merupakan sebagian kecil dari keseluruhan kata yang ada pada buku tersebut. Untuk menyederhanakan operasi modul ini, kita akan mengabaikan tanda baca dan huruf besar.\n",
    "\n",
    "Fungsi `read_time_machine()` akan memuat dataset dari buku tersebut menjadi sebuah array yang berisi kata-kata dari tiap baris. Fungsi tersebut juga akan mengabaikan huruf besar dan kecil serta tanda baca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset akan dipecah menjadi beberapa baris yang akan disimpan dalam array `lines`. Kita akan coba memeriksa panjang array `lines` untuk mengetahui jumlah baris yang ada dan kita akan mencetak beberapa contoh baris dari dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# text lines: 3221\n",
      "the time machine by h g wells\n",
      "to stretch through centuries at last a steady twilight brooded over\n"
     ]
    }
   ],
   "source": [
    "print(f'# text lines: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[random.randint(0,3000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisasi\n",
    "\n",
    "Tokenisasi merupakan fungsi yang akan menerima input berupa `lines` dan memecah masukan tersebut menjadi token. Token adalah unit dasar dari sebuah teks berupa string kata atau karakter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['were', 'three', 'dimensional', 'representations', 'of', 'his', 'four', 'dimensioned']\n",
      "['being', 'which', 'is', 'a', 'fixed', 'and', 'unalterable', 'thing']\n",
      "[]\n",
      "['scientific', 'people', 'proceeded', 'the', 'time', 'traveller', 'after', 'the', 'pause']\n",
      "['required', 'for', 'the', 'proper', 'assimilation', 'of', 'this', 'know', 'very', 'well', 'that']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):  #@save\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "\n",
    "'''\n",
    "Mencetak contoh token\n",
    "'''\n",
    "\n",
    "for i in range(100,105):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary\n",
    "\n",
    "Token berupa string atas kata maupun karakter pun masih belum memadai untuk dijadikan masukan bagi sebuah model. Perlu diingat bahwa dalam deep learning, model menerima masukan berupa vektor yang berisi nilai numerik sehingga kita harus membuat sebuah kamus kata (dictionary) yang menyatakan setiap token yang ada. Kamus kata ini akan menjadi sebuah array yang berisi setiap token yang ada pada dataset. Kamus kata atau dictionary juga kerap disebut sebagai vocabulary\n",
    "\n",
    "Kamus kata akan berisi daftar kata (tokens) dan frekuensi kemunculannya. Kemudian, setiap kata akan diberi sebuah index numerik yang unik bergantung pada frekuensi kemunculannya, daftar dari kata ini disebut dengan corpus. Tokens yang jarang muncul tidak akan disertakan dalam dictionary supaya mengurangi kompleksitas. Token yang tidak ada dalam corpus tidak akan dijadikan masukan model dan akan dipetakan dengan `<unk>`.\n",
    "\n",
    "Kamus kata ini akan dibuat dalam bentuk kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KamusKata:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 tokens=None,\n",
    "                 min_freq=0,\n",
    "                 reserved_tokens=None\n",
    "                 ):\n",
    "        \n",
    "        if tokens is None: # jika token kosong, maka token akan dijadikan list kosong\n",
    "            tokens = []\n",
    "            \n",
    "        if reserved_tokens is None: # jika reserved_tokens kosong, maka reserved_tokens akan dijadikan list kosong\n",
    "            reserved_tokens = []\n",
    "            \n",
    "        counter = count_corpus(tokens) # menghitung jumlah kata pada token\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # mengurutkan berdasarkan frekuensi\n",
    "        \n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens # apabila ada unknown token, maka indeksnya akan dijadikan 0\n",
    "        \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} # apabila ada unknown token, maka indeksnya akan dijadikan 0\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,tokens):\n",
    "        if not isinstance(tokens,(list,tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.token_to_idx.get(token, self.unk) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "        \n",
    "def count_corpus(tokens): \n",
    "        if len(tokens) == 0 or isinstance(tokens[0], list): # jika tokens kosong atau tokens pada indeks 0 adalah list:\n",
    "            tokens = [token for line in tokens for token in line] # maka tokens akan dijadikan array 1 dimensi ??? [TODO]\n",
    "        return collections.Counter(tokens) # menghitung jumlah kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penjelasan: Ketika sebuah Objek dibuat, maka akan terdapat beberapa atribut seperti:\n",
    "- `tokens`: berisi daftar kata yang ada pada corpus\n",
    "- `min_freq`: nilai minimum frekuensi dari kata yang akan dijadikan kamus kata\n",
    "- `reserved_tokens`: berisi daftar token yang tidak akan dijadikan kamus kata\n",
    "\n",
    "Silahkan lihat contoh corpus di bawah ini\n",
    "\n",
    "### Contoh Penggunaan KamusKata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, 'amir': 1, 'nama': 2, 'saya': 3, 'budi': 4, 'dia': 5, 'jessica': 6, 'teman': 7, 'siapa?': 8, '????': 9, '-': 10}\n"
     ]
    }
   ],
   "source": [
    "contoh_tokens = [['nama','saya','budi'],['nama','dia','amir'],['jessica','teman','amir'],['amir','siapa?'],['????'],[],['-']]\n",
    "\n",
    "vocab = KamusKata(contoh_tokens)\n",
    "print(vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apabila `min_freq=2`, maka hanya kata yang muncul minimal 2 kali yang akan dijadikan corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, 'amir': 1, 'nama': 2}\n",
      "['<unk>', 'amir', 'nama']\n"
     ]
    }
   ],
   "source": [
    "vocab = KamusKata(contoh_tokens, min_freq=2)\n",
    "print(vocab.token_to_idx)\n",
    "print(vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vocab['nama'])   # mencari indeks dari 'nama'\n",
    "print(vocab['susilo']) # mencari indeks dari 'susilo' -> Nilai kembalian pasti menjadi 0 atau <unk> karena 'Susilo' tidak ada dalam corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan KamusKata\n",
    "Sekarang mari kita gunakan `tokens` dan kita masukkan ke kelas KamusKata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = KamusKata(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10]) # menampilkan 10 token teratas dan dijadikan tuple -> ('tokennya', indeksnya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "Indeks [1, 19, 71, 16, 37, 11, 115, 42, 680, 6, 586, 4, 108]\n",
      "Kata ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "Indeks [7, 1420, 5, 2185, 587, 6, 126, 25, 330, 127, 439, 3]\n",
      "Kata ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "Indeks [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range (8,11):                      # mencetak corpus ke 8 sampai ke 10\n",
    "    print(f'Kata', tokens[i])\n",
    "    print(f'Indeks', vocab[tokens[i]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81b9ccaf5ef21c8a6faa6d42f6e42fcf9eafd7625a2befdd601079168fccee32"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
