{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN pada Teks\n",
    "\n",
    "Pada modul sebelumnya, kita telah membahas bagaimana memprediksi data sekuensial. Data dapat berwujud apa saja dan RNN merupakan sebuah bentuk model yang mendukung pemrosesan data sekuensial. Kali ini kita akan mencoba RNN pada bentuk data sekuensial lainnya yaitu teks.\n",
    "\n",
    "### Library yang di Import\n",
    "- `collections` digunakan untuk menyediakan beberapa container datatypes tambahan\n",
    "- `re` digunakan untuk operasi regular expression (regex) pada python\n",
    "- `d2l` digunakan untuk mengimport library yang dibutuhkan dalam modul ini yang bersumber dari buku [d2l.ai](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memuat Dataset\n",
    "\n",
    "Pada modul ini, dataset yang digunakan bersumber dari sebuah buku [The Time Machine, oleh H.G. Wells](http://www.gutenberg.org/ebooks/35). Dataset yang akan digunakan berjumlah 30000 kata dan merupakan sebagian kecil dari keseluruhan kata yang ada pada buku tersebut. Untuk menyederhanakan operasi modul ini, kita akan mengabaikan tanda baca dan huruf besar.\n",
    "\n",
    "Fungsi `read_time_machine()` akan memuat dataset dari buku tersebut menjadi sebuah array yang berisi kata-kata dari tiap baris. Fungsi tersebut juga akan mengabaikan huruf besar dan kecil serta tanda baca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset akan dipecah menjadi beberapa baris yang akan disimpan dalam array `lines`. Kita akan coba memeriksa panjang array `lines` untuk mengetahui jumlah baris yang ada dan kita akan mencetak beberapa contoh baris dari dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# text lines: 3221\n",
      "the time machine by h g wells\n",
      "the fact is that insensibly the absolute strangeness of everything\n"
     ]
    }
   ],
   "source": [
    "print(f'# text lines: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[random.randint(0,3000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisasi\n",
    "\n",
    "Tokenisasi merupakan fungsi yang akan menerima input berupa `lines` dan memecah masukan tersebut menjadi token. Token adalah unit dasar dari sebuah teks berupa string kata atau karakter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['were', 'three', 'dimensional', 'representations', 'of', 'his', 'four', 'dimensioned']\n",
      "['being', 'which', 'is', 'a', 'fixed', 'and', 'unalterable', 'thing']\n",
      "[]\n",
      "['scientific', 'people', 'proceeded', 'the', 'time', 'traveller', 'after', 'the', 'pause']\n",
      "['required', 'for', 'the', 'proper', 'assimilation', 'of', 'this', 'know', 'very', 'well', 'that']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):  #@save\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "\n",
    "'''\n",
    "Mencetak contoh token\n",
    "'''\n",
    "\n",
    "for i in range(100,105):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary\n",
    "\n",
    "Token berupa string atas kata maupun karakter pun masih belum memadai untuk dijadikan masukan bagi sebuah model. Perlu diingat bahwa dalam deep learning, model menerima masukan berupa vektor yang berisi nilai numerik sehingga kita harus membuat sebuah kamus kata (dictionary) yang menyatakan setiap token yang ada. Kamus kata ini akan menjadi sebuah array yang berisi setiap token yang ada pada dataset. Kamus kata atau dictionary juga kerap disebut sebagai vocabulary\n",
    "\n",
    "Kamus kata akan berisi daftar kata (tokens) dan frekuensi kemunculannya. Kemudian, setiap kata akan diberi sebuah index numerik yang unik bergantung pada frekuensi kemunculannya, daftar dari kata ini disebut dengan corpus. Tokens yang jarang muncul tidak akan disertakan dalam dictionary supaya mengurangi kompleksitas. Token yang tidak ada dalam corpus tidak akan dijadikan masukan model dan akan dipetakan dengan `<unk>`.\n",
    "\n",
    "Kamus kata ini akan dibuat dalam bentuk kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KamusKata:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 tokens=None,\n",
    "                 min_freq=0,\n",
    "                 reserved_tokens=None\n",
    "                 ):\n",
    "        \n",
    "        if tokens is None: # jika token kosong, maka token akan dijadikan list kosong\n",
    "            tokens = []\n",
    "            \n",
    "        if reserved_tokens is None: # jika reserved_tokens kosong, maka reserved_tokens akan dijadikan list kosong\n",
    "            reserved_tokens = []\n",
    "            \n",
    "        counter = count_corpus(tokens) # menghitung jumlah kata pada token\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # mengurutkan berdasarkan frekuensi\n",
    "        \n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens # apabila ada unknown token, maka indeksnya akan dijadikan 0\n",
    "        \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} # apabila ada unknown token, maka indeksnya akan dijadikan 0\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,tokens): # mengambil item dari token\n",
    "        if not isinstance(tokens,(list,tuple)): # jika tokens bukan list atau tuple\n",
    "            return self.token_to_idx.get(tokens, self.unk) # maka akan mengembalikan indeks unknown\n",
    "        return [self.token_to_idx.get(token, self.unk) for token in tokens] # mengambil item dari indeks\n",
    "    \n",
    "    def to_tokens(self, indices): # mengubah indeks menjadi token\n",
    "        if not isinstance(indices, (list, tuple)): # jika indices bukan list atau tuple\n",
    "            return self.idx_to_token[indices] # maka akan mengembalikan token\n",
    "        return [self.idx_to_token[idx] for idx in indices] # mengambil item dari indeks\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "        \n",
    "def count_corpus(tokens): \n",
    "        if len(tokens) == 0 or isinstance(tokens[0], list): # jika tokens kosong atau tokens pada indeks 0 adalah list:\n",
    "            tokens = [token for line in tokens for token in line] # maka tokens akan dijadikan array 1 dimensi ??? [TODO]\n",
    "        return collections.Counter(tokens) # menghitung jumlah kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penjelasan: Ketika sebuah Objek dibuat, maka akan terdapat beberapa atribut seperti:\n",
    "- `tokens`: berisi daftar kata yang ada pada corpus\n",
    "- `min_freq`: nilai minimum frekuensi dari kata yang akan dijadikan kamus kata\n",
    "- `reserved_tokens`: berisi daftar token yang tidak akan dijadikan kamus kata\n",
    "\n",
    "Silahkan lihat contoh penggunaan dictionary di bawah ini\n",
    "\n",
    "### Contoh Penggunaan KamusKata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, 'amir': 1, 'nama': 2, 'saya': 3, 'budi': 4, 'dia': 5, 'jessica': 6, 'teman': 7, 'siapa?': 8, '????': 9, '-': 10}\n"
     ]
    }
   ],
   "source": [
    "contoh_tokens = [['nama','saya','budi'],['nama','dia','amir'],['jessica','teman','amir'],['amir','siapa?'],['????'],[],['-']]\n",
    "\n",
    "vocab = KamusKata(contoh_tokens)\n",
    "print(vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apabila `min_freq=2`, maka hanya kata yang muncul minimal 2 kali yang akan dijadikan dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, 'amir': 1, 'nama': 2}\n",
      "['<unk>', 'amir', 'nama']\n"
     ]
    }
   ],
   "source": [
    "vocab = KamusKata(contoh_tokens, min_freq=2)\n",
    "print(vocab.token_to_idx)\n",
    "print(vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vocab['nama'])   # mencari indeks dari 'nama'\n",
    "print(vocab['susilo']) # mencari indeks dari 'susilo' -> Nilai kembalian pasti menjadi 0 atau <unk> karena 'Susilo' tidak ada dalam corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan KamusKata\n",
    "Sekarang mari kita gunakan `tokens` dan kita masukkan ke kelas KamusKata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = KamusKata(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10]) # menampilkan 10 token teratas dan dijadikan tuple -> ('tokennya', indeksnya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "Indeks [1, 19, 71, 16, 37, 11, 115, 42, 680, 6, 586, 4, 108]\n",
      "Kata ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "Indeks [7, 1420, 5, 2185, 587, 6, 126, 25, 330, 127, 439, 3]\n",
      "Kata ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "Indeks [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range (8,11):                      # mencetak corpus ke 8 sampai ke 10\n",
    "    print(f'Kata', tokens[i])\n",
    "    print(f'Indeks', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan Seluruh Fungsi Diatas\n",
    "\n",
    "Fungsi `load_corpus_time_machine` akan melakukan:\n",
    "- Membaca dataset dari buku tersebut\n",
    "- Membuat `tokens` berdasarkan dataset\n",
    "- Membuat `dictionary` berdasarkan `tokens`, terdiri atas karakter\n",
    "- Membuat corpus berdasarkan setiap line dari tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170580 28\n",
      "[3, 9, 2, 1, 3, 5, 13, 2, 1, 13]\n",
      "[(' ', 29927), ('e', 17838), ('t', 13515), ('a', 11704), ('i', 10138), ('n', 9917), ('o', 9758), ('s', 8486), ('h', 8257), ('r', 7674)]\n"
     ]
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    \n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = KamusKata(tokens)\n",
    "    \n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "print(len(corpus), len(vocab))\n",
    "\n",
    "print(corpus[:10])\n",
    "print(vocab.token_freqs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implementasi RNN menggunakan Teks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mengimpor library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mempersiapkan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_steps = 35\n",
    "corpus, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inisiasi Parameter Model\n",
    "Pada modul ini, model RNN akan diinisiasi secara manual tanpa menggunakan layer nn dari torch. Berikut penjelasan dari setiap layer yang digunakan:\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"../assets/rnn.jpeg\", width=\"500\">\n",
    "\n",
    "Sumber: [d2l.ai](https://d2l.ai/)\n",
    "</div>\n",
    "\n",
    "- W adalah Weight: nilai bobot dari tiap neuron\n",
    "- b adalah Bias: nilai bias dari tiap neuron\n",
    "- X adalah Input: Input yang digunakan pada modul ini berupa teks\n",
    "- h adalah Hidden State: nilai hidden state dari tiap neuron yang dipengaruhi berdasarkan hidden state sebelumnya\n",
    "- q adalah Output: Output dari tiap neuron yang dipengaruhi berdasarkan hidden state saat ini dan input\n",
    "\n",
    "Contoh:\n",
    "- W_xh = Weight dari input ke hidden state\n",
    "- W_hh = Weight dari hidden state ke hidden state\n",
    "dan seterusnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def get_params(vocab_size, hidden_size, device):\n",
    "    input_size = output_size = vocab_size\n",
    "    \n",
    "    # Hidden Layer\n",
    "    W_xh    = torch.randn((input_size, hidden_size), device=device)\n",
    "    W_hh    = torch.randn((hidden_size, hidden_size), device=device)\n",
    "    b_h     = torch.zeros(hidden_size, device=device)\n",
    "    \n",
    "    # Output Layer\n",
    "    W_hq    = torch.randn((hidden_size, output_size), device=device)\n",
    "    b_q     = torch.zeros(output_size, device=device)\n",
    "    \n",
    "    # Mengaktifkan semua gradient\n",
    "    \n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch:\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 ):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = vocab_size\n",
    "        self.output_size = vocab_size\n",
    "        \n",
    "         # Hidden Layer\n",
    "        self.W_xh    = torch.randn((self.input_size, self.hidden_size))\n",
    "        self.W_hh    = torch.randn((self.hidden_size,self.hidden_size))\n",
    "        self.b_h     = torch.zeros(self.hidden_size)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.W_hq    = torch.randn((self.hidden_size, self.output_size))\n",
    "        self.b_q     = torch.zeros(self.output_size)\n",
    "        \n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81b9ccaf5ef21c8a6faa6d42f6e42fcf9eafd7625a2befdd601079168fccee32"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
